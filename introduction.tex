Haskell programmers enjoy the benefits of strong static types and purity:
static types eliminate many bugs early on in the development cycle, and purity
simplifies equational reasoning about programs. Despite these benefits, however,
bugs may still remain in purely functional code and programs often
crash if applied to the wrong arguments.
For example, consider these Haskell definitions:
\begin{code}
  f xs = head (reverse (True : xs))
  g xs = head (reverse xs)
\end{code}
Both @f@ and @g@ are well typed (and hence do not ``go wrong'' in Milner's
sense), but @g@ will crash when applied to the empty list, whereas @f@
cannot crash regardless of its arguments.
To distinguish the two we need reasoning that goes well beyond
that typically embodied in a standard type system.

Many variations of {\em dependent type systems}~\cite{norell:thesis,Xi:2007:DMA:1230756.1230759,fstar} or
{\em refinement type systems}~\cite{Rondon:2008:LT:1375581.1375602,Knowles+:sage}
have been proposed to address this problem, each offering different degrees of
expressiveness or automation.
Another line of work aiming to address this challenge, studied by many researchers
as well~\cite{Findler:2002:CHF:581478.581484,Blume:2006:SCM:1166013.1166016,Knowles+:sage,Siek06gradualtyping,Wadler:2009:WPC:1532974.1532976}, allows programmers to annotate
functions with {\em contracts}, which are forms of behavioural specifications.
For instance, we might write the following contract for
@reverse@:
\[ @reverse@ \in (xs : \CF) -> \{ ys \mid @null@\;xs\; @<=>@\; @null@\;ys \}  \]
%% \begin{code}
%%   reverse ::: xs:CF -> { ys | null xs <=> null ys }
%% \end{code}
This contract annotation asserts that if @reverse@ is applied to a
crash-free (@CF@) argument list @xs@ then the result @ys@ will be empty (@null@)
if and only if @xs@ is empty. What is a crash-free argument?
Since we are using lazy semantics, a list could contain cons-cells that yield
errors when evaluated, and the @CF@ precondition asserts that the input list
is not one of those.
Notice also that @null@ and @<=>@ are just ordinary Haskell functions, perhaps
written by the programmer, even though they appear inside contracts.

With this property of @reverse@ in hand, we might
hope to prove that @f@ satisfies the contract
\[ @f@ \in \CF -> \CF \]
But how do we verify that @reverse@ and @f@ satisfy the claimed
contracts? Contracts are often tested dynamically, but
our plan here is different: we want to verify contracts \emph{statically}
and \emph{automatically}.

It should be clear that there is a good deal of logical reasoning to do,
and a now-popular approach is to delegate the task to an off-the-shelf theorem
prover such as Z3~\cite{z3citation} or Vampire~\cite{vampire}, or search for
counterexamples with a finite model finder~\cite{paradox}.
With that in mind, we make the following new contributions:

\begin{itemize}
  \item We give a translation of Haskell programs to first-order logic (FOL) theories.
        It turns out that lazy programs (as opposed to
        strict ones!) have a very natural translation into first-order logic
        (Section~\ref{ssect:trans-fol}).
  \item We give a translation of contracts to FOL formulae, and an axiomatisation of
        the language semantics in FOL
        (Section~\ref{s:contracts-fol}).
  \item Our main contribution is to show that if we can prove the formula
        that arises from a contract translation
        for a given program, then the program does indeed satisfy this contract. Our proof
        uses the novel idea of employing the denotational
        semantics as a first-order model (Section~\ref{ssect:denot}).
  \item We show how to use this translation in practice for static contract checking with
        a FOL theorem prover (Section~\ref{sect:soundness}),
        and how to prove goals by induction (Section~\ref{sect:extensions}).
% \item \dr{TODO} The other main contribution is a sound and complete
%   heuristic called ``Minimization'', that indeed minimizes the search
%   space for theorem provers (Section~\ref{sect:min}), which by
%   practical experience yields finite counterexamples in many cases,
%   and greatly enhances up proving times. We also investigate how to
%   express it using triggers.
\end{itemize}

This work is a first step towards practical contract checking
for Haskell programs, laying out the theoretical foundations for further engineering
and experimentation. Nevertheless, we have already implemented a prototype for Haskell
programs that uses GHC as a front-end. We have evaluated the practicality of our approach
on many examples, including lazy and higher-order programs, and goals that require
induction. We report this initial encouraging evaluation in
Section~\ref{sect:implementation}.

To our knowledge no one has previously presented a translation of lazy higher-order programs to
first-order logic, in a provably sound way with respect to a denotational
semantics. Furthermore, our approach to static contract checking is
distinctly different to previous work: instead of wrapping and
symbolic execution~\cite{xu+:contracts,Xu:2012:HCC:2103746.2103767},
we harness purity and laziness to directly use the denotational semantics
of programs and contracts and discharge the obligations with a
FOL theorem prover, side-stepping
the wrapping process. Instead of generating verification conditions by pushing
pre- and post- conditions through a program, we directly ask a theorem prover to prove
a contract for the FOL encoding of a program.
We discuss related work in Section~\ref{sect:related}.

%% \newpage
%%   \item The translation
%% For this paper we focus on the translation, but to substantiate the practicality


%% \end{itemize}


%% \begin{itemize}
%% \item We show how to translate Haskell terms
%% into First Order Logic (FOL) (Section~\ref{ssect:trans-fol}).
%% It may appear surprising that this
%% is even possible, since Haskell is a higher order language.  Although
%% the basic idea of the translation is folklore in the community,
%% we believe that this paper is the first to explain it explicitly.

%% \item We also show how to translate \emph{contracts} into FOL
%%       (Section~\ref{s:contracts-fol}),
%%       a translation that is rather less obvious.

%% \item We give a proof based on denotational semantics
%% that if the FOL prover discharges a
%% suitable theorem about the translated Haskell term and contract,
%% then indeed the original Haskell term satisfies that contract (Section~\ref{s:xxx}).

%% %\item It is one thing to make a sound translation, and quite another
%% %to produce FOL terms that the FOL prover can actually prove anything
%% %about --- a common experience is that it goes out to lunch instead.  We
%% %describe a number of techniques that dramatically improve
%% %theorem-proving times, moving them from infeasible to feasible (Section\ref{xxx}).

%% \item For this paper we focus on the
%% translation, but we have also implemented a static contract checker
%% for Haskell itself, by using GHC as a front end.  We have evaluated
%% the practicality of this approach on many examples, including lazy and
%% higher-order programs, as we describe in Section~\ref{xxx}.  \spj{I'd like
%% to say something more substantial here.}
%% \end{itemize}



















