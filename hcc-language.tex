To formalize the ideas behind our implementation, we define a
tiny source language $\cal L$:
polymorphic, higher-order, call-by-name $\lambda$-calculus with
algebraic datatypes, pattern matching, and recursion.  
Our actual implementation treats all of Haskell, by using GHC as a front 
end to translate Haskell into language $\cal L$.

\subsection{Syntax of $\cal L$} \label{s:syntax}

\begin{figure}
\[\begin{array}{l} 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Programs, definitions, and expressions}} \\
P   & ::= & d_1 \ldots d_n \\
d   & ::= & f\; \ol{a} \; \ol{(x\!:\!\tau)} = u \\
u   & ::= & e \; \mid \; @case@\;e\;@of@\;\ol{K\;\ol{y} -> e}
\end{array}
\\ 
\begin{array}{lrll}
e  & ::=  & x            & \text{Variables} \\ 
   & \mid & f[\ol{\tau}] & \text{Function variables} \\ 
   & \mid & K[\ol{\tau}](\ol{e}) & \text{Data constructors (fully applied)} \\
   & \mid & e\;e         & \text{Applications} \\
   & \mid & @BAD@        & \text{Runtime error} \\ 
\end{array}\\ \\ 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Syntax of closed values}} \\
 v,w & ::= & K^\ar[\ol{\tau}](\oln{e}{\ar}) \;\mid\; f^\ar[\ol{\tau}]\;\oln{e}{m < \ar} \;\mid\; @BAD@ \\ \\ 
\end{array}
\\ 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Contracts}} \\
 \Ct & ::=  & \{ x \mid e \}        & \text{Base contracts}  \\ 
     & \mid &  (x : \Ct_1) -> \Ct_2      & \text{Arrow contracts} \\ 
     & \mid & \Ct_1 \& \Ct_2             & \text{Conjunctions}   \\ 
     & \mid & \CF                        & \text{Crash-freedom}   \\
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Types}} \\
\tau,\sigma & ::=  & T\;\taus & \text{Datatypes} \\ 
            & \mid & a \mid \tau -> \tau 
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Type environments and signatures}} \\
\Gamma & ::=  & \cdot \mid \Gamma,x \\
\Delta & ::=  & \cdot \mid \Delta,a \mid \Delta,x{:}\tau \\
\Sigma & ::=  & \cdot \mid \Sigma,T{:}n \mid \Sigma,f{:}\forall\ol{a} @.@ \tau \mid \Sigma,K^{\ar}{:}\forall\ol{a} @.@ \oln{\tau}{\ar} -> @T@\;\as
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Auxiliary functions}} \\
%% constrs(\Sigma,T) & = & \{ K \mid (K{:}\forall \as @.@ \taus -> T\;\as) \in \Sigma \} \\
(\cdot)^{-}            & = & \cdot \\
(\Delta,a)^{-}         & = & \Delta^{-} \\
(\Delta,(x{:}\tau)^{-} & = & \Delta^{-},x
%% \tyar{D}{f} & = & n & \\ 
%%             & \multicolumn{3}{l}{\text{when}\; (f |-> \Lambda\oln{a}{n} @.@ \lambda\ol{x{:}\tau} @.@ u) \in D} \\
%% \tmar{D}{f} & = & n & \\ 
%%             & \multicolumn{3}{l}{\text{when}\; (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{n} @.@ u) \in D}
\end{array}
\end{array}\] 
\caption{Syntax of $\cal L$ and its contracts}\label{fig:syntax}
\end{figure}

Figure~\ref{fig:syntax} presents the syntax of $\cal L$.  A program
$P$ consists of a set of recursive function definitions $d_1 \ldots
d_n$. Each definition has a left hand side that binds its type-variable and
term-variable parameters;
if $f$ has $n$ term-variable parameters we say that 
it has arity $n$, and sometimes write it $f^n$.
The right hand side $u$ of a definition is either a @case@ expression or a
@case@-free expression $e$.  A @case@-free expression consists of
variables $x$, function variables $f[\taus]$ fully applied to their
type arguments, applications $e_1\;e_2$, data constructor applications
$K[\taus](\ol{e})$, as well as the special value @BAD@, which will be
used to model failure as a throwable error term. As a notation, we use
$\oln{x}{n}$ for sequences of elements of size $n$. When $n$ is
ommitted $\ol{x}$ has a size which is implied by the context or is not
interesting.

Our language embodies several convenient syntactic constraints: (i)~$\lambda$
abstractions occur only at the top-level, (ii)~@case@-expressions can
only immediately follow a function definition, and (iii) constructors
are fully applied.  These constraints do not restrict expressiveness;
lambda-lifting, @case@-lifting, and eta-expansion respectively can
easily transform a program with nested constructs and
partially-applied constructors into our restricted form.  Indeed our
prototype relies on existing implementation of similar transformations
from the GHC-as-a-library API. However this simpler language is
instead extremely convenient for the translation of programs to
first-order logic.

Figure~\ref{fig:syntax} embodies one other inessential choice in order to 
facilitate the formalization and implementation: we assume that 
functions have arity at least one (disallowing CAF's).  \spj{Disallowing CAFs is quite significant.
We need to say more about why.} \dv{It is not essential -- it was in some previous version of a proof. I will fix this.}

$\cal L$ is an explicitly-typed language, and we assume the 
existence of a typing relation $\Sigma |- P$, which checks 
that a program conforms to the definitions in the signature $\Sigma$. A signature $\Sigma$ (Figure~\ref{fig:syntax})
records the declared data types, data constructors and types of functions in the program $P$. The 
well-formedness of expressions is checked with a typing relation $\Sigma;\Delta |- u : \tau$, where $\Delta$
is a typing environment, also in Figure~\ref{fig:syntax}.
We do not give the details of the typing relation since it is standard. An additional property that we require 
from the typing relation is that it {\em asserts the exhaustiveness of pattern matches}. In an {\em actual}
source language programmers may ommit pattern matches but here we will assume that all pattern matches 
are exhaustive. Originally-incomplete cases have been completed to return the crashing term @BAD@. For 
instance, the program:
\begin{code}
head :: [a] -> [a]
head (x:xs) = x
\end{code}
will be represented in our language as:
\[\begin{array}{l}
   @head@\; a\; (x{:}[a]) = @case@\;x\;@of@ \{ [] -> @BAD@ ; (x:xs) -> x \} 
\end{array}\]
Finally, our technical development and analysis in the following sections assume that programs have been 
checked for type errors. 

The syntax of closed values is also given in Figure~\ref{fig:syntax}. Since we do not 
have arbitrary $\lambda$-abstractions, values can only be partial function applications
$f^\ar[\ol{\tau}]\;\oln{e}{m < \ar}$, data constructor applications $K[\tau](\ol{e})$, 
and the error term @BAD@. 

\subsection{Operational semantics of $\cal L$}

\spj{Why do we give an operational as well as denotational semantics?}
The big-step operational semantics of our language is given in 
Figure~\ref{fig:opsem}, which contains no surprises. One interesting
detail of big-step semantics is that they do not distinguish between non-termination 
and ``getting stuck'', meaning that if $P \not|- e \Downarrow v$ then $e$ could either diverge or its 
evaluation could get stuck. We return to this convenient for our purposes form of operational 
semantics later. \spj{Where later? Also this sentence is hard to parse; indeed I'm not quite
sure what it means.}
\begin{figure}
\[\begin{array}{c} 
\ruleform{P |- u \Downarrow v} \\ 
\prooftree
\begin{array}{c} \ \\ 
\end{array}
-------------------------------------{EVal}
P |- v \Downarrow v
~~~~~
\begin{array}{c}
(f \;\ol{a}\;\oln{(x{:}\tau)}{m} = u) \in P \\
P |- u[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \Downarrow v
\end{array}
-------------------------------------{EFun}
P |- f[\ol{\tau}]\;\oln{e}{m} \Downarrow v
~~~~~
\begin{array}{c}  
P |- e_1 \Downarrow v_1 \\
P |- v_1\;e_2 \Downarrow w
\end{array}
------------------------------------------------{EApp}
P |- e_1\;e_2 \Downarrow w
~~~~~
\begin{array}{c}  
P |- e_1 \Downarrow @BAD@ 
\end{array}
------------------------------------------------{EBadApp}
P |- e_1\;e_2 \Downarrow @BAD@
~~~~~
%% \endprooftree \\ \\ 
%% \ruleform{P |- u \Downarrow v} \\ \\
%% \prooftree
%% P |- e \Downarrow v
%% -------------------------------------{EUTm}
%% P |- e \Downarrow v
%% ~~~~ 
\begin{array}{c}
P |- e \Downarrow K_i[\ol{\sigma}_i](\ol{e}_i) \quad
P |- e'_i[\ol{e}_i/\ol{y}_i] \Downarrow w
\end{array}
------------------------------------{ECase}
P |- @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'} \Downarrow w
~~~~~
\begin{array}{c}
P |- e \Downarrow @BAD@ \\
\end{array}
------------------------------------{EBadCase}
P |- @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'} \Downarrow @BAD@
%% \begin{array}{c}
%% (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{m} @.@ @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}) \in D \\
%% D |- e[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \Downarrow @BAD@ \\
%% \end{array}
%% -------------------------------------{EBadCase}
%% D |- f[\ol{\tau}]\;\oln{e}{m} \Downarrow @BAD@
\endprooftree
\end{array}\]
\caption{Operational semantics of $\cal L$}\label{fig:opsem}
\end{figure}
%% We can state some standard properties of the typing and evaluation relation.
%% \begin{lemma}[Subject reduction]
%% Assume $\Sigma |- P$ and $\Sigma;\cdot |- e : \tau$
%% If $P |- e \Downarrow w$ then $P |- value(w)$ and $\Sigma;\cdot |- w : \tau$.
%% \end{lemma}
The operational semantics of Figure~\ref{fig:opsem} has the possibility of non-deterministism because
of the overlapping of several rules for applications. But that is not a problem, as we can prove that evaluation 
is deterministic using the following two lemmas.
\begin{lemma}[Value determinacy]
If $\Sigma;\cdot |- v : \tau$ and 
$\Sigma |- P$ and $P |- v \Downarrow w$ then $ v = w $.
\end{lemma}
\begin{lemma}[Determinacy of evaluation]
If $\Sigma;\cdot |- e : \tau$ and 
$\Sigma |- P$ and $\Sigma;\cdot |- e \Downarrow v_1$ and $\Sigma;\cdot |- e \Downarrow v_2$ then
$v_1 = v_2$.
\end{lemma}
Finally, big-step soundness asserts that an expression that evaluates results in a
well-typed value.
\begin{lemma}[Big-step soundness]
If $\Sigma;\cdot |- e : \tau$ and 
$\Sigma |- P$ and $\Sigma;\cdot |- e \Downarrow v$ then $\Sigma;\cdot |- v : \tau$.
\end{lemma}

%% Figure~\ref{fig:syntax} also presents the syntax of {\em contracts} to
%% keep all essential definitions grouped together, but in this section
%% we will only focus on the language and its semantics. In
%% Section~\ref{sect:contracts} we return to the syntax and semantics of
%% contracts.

%% \subsection{Denotational semantics}\label{ssect:denot}

%% Having presented the language $\cal L$ and its operational semantics, we now return to 
%% our roadmap which is to axiomatize and use the denotational semantics of programs to 
%% perform static contract checking. In what follows we will assume a program $P$, 
%% well-formed in a signature $\Sigma$, so that $\Sigma |- P$. Most of what follows is 
%% an adaptation of folklore techniques to our setting and there are no 
%% surprises -- we refer the reader to~\cite{winskel} or~\cite{benton+:coq-domains} 
%% for a short and modern exposition of the standard methodology.

%% Given a signature $\Sigma$ we define a strict bi-functor $F$ on complete partial 
%% orders (cpos), below:
%% %% For a well-formed signature $\Sigma$, we define the strict bi-functor on cpos, below, 
%% %% assuming that $K_1\ldots K_k$ are all the constructors in $\Sigma$: 
%% \[\begin{array}{lclll}
%%   F(D^{-},D^{+}) & = & ( \quad{\prod_{\ar_1}{D^{+}}} & K_1^{\ar_1} \in \Sigma \\
%%                & + & \;\quad\ldots                    & \ldots \\
%%                & + & \;\quad{\prod_{\ar_k}{D^{+}}} & K_k^{\ar_k} \in \Sigma \\ 
%%                & + & \;\quad(D^{-} =>_c D^{+}) \\
%%                & + & \;\quad\unitcpo_{bad} \quad )_{\bot}
%% \end{array}\]
%% \spj{Why bi-functor? Why not just functor?}%% \dv{I am using Pitts' and Nick Benton's formulation and this 
%% %% thing needs to be a bifunctor. It is not a functor if you have contra-varianance.} 
%% The bi-functor $F$ is the lifting of a big sum: that sum consists of 
%% (i) products, one for each possible constructor (even across different data types), (ii) the continuous
%% function space from $D^{-}$ to $D^{+}$, and (iii) a unit cpo to denote @BAD@ values. 
%% The notation $\prod_{n}{D}$ abbreviates $n$-ary products of cpos (the unit cpo $\unitcpo$ if $n = 0$). 
%% The product and sum constructions are standard, but note that we use their non-strict versions. 
%% The notation $C =>_c D$ denotes the cpo 
%% induced by the space of continuous functions from the cpo $C$ to the cpo $D$. We use 
%% the notation $\unitcpo_{bad}$ to 
%% denote a single-element cpo -- the $bad$ subscript is just there for readability. 
%% The notation $D_\bot$ is {\em lifting}, which is a monad, equipped with the following two continuous 
%% functions.
%% \[\begin{array}{l}
%%    \ret   : D =>_c D_\bot \\ 
%%    \bind_{f : D =>_c E_\bot} : D_\bot =>_c E_\bot
%% \end{array}\]
%% with the obvious definitions. 

%% Observe that we have dropped all type information from the source language. The elements of the products 
%% corresponding to data constructors are simply $D^{+}$ (instead of more a precise description from type 
%% information) and the return types of data constructors are similarly ignored. This is not to say that 
%% a more type-rich denotational semantics is not possible (or desirable even) but this simple denotational 
%% semantics turns out to be sufficient for formalization and verification. 

%% %% for $\lambda$-abstractions and @BAD@. Observe that we have 
%% %% Moreover, the following continuous operations are defined:
%% %% \[\begin{array}{l} 
%% %%    \curry_{f : D\times E =>_c F} : D =>_c (E =>_C F) \\ 
%% %%    \eval : (E =>_c D)\times E =>_c D 
%% %% \end{array}\] 
%% %% for any cpos $D, E, F$.

%% \spj{What is ``the equation induced by F''?  Maybe $$D_{\infty} = F( D_{\infty}, D_{\infty})$$? 
%% Let's write it down.}
%% Using the standard {\em embedding-projection} pairs methodology we can show the following.
%% \begin{lemma}\label{lem:rec-solution} 
%% There exists a solution to the domain-recursive equation induced by $F$, call it $D_{\infty}$.
%% Moreover, let a value-domain: $V_{\infty}$ be defined as:
%%     \[V_{\infty} = \begin{array}[t]{ll}
%%              \quad\;{\prod_{\ar_1}{D_{\infty}}} & K_1^{\ar_1} \in \Sigma \\
%%              \; + \;\ldots                    & \ldots \\
%%              \; + \;{\prod_{\ar_k}{D_{\infty}}} & K_k^{\ar_k} \in \Sigma \\ 
%%              \; + \;(D_{\infty} =>_c D_{\infty}) \\
%%              \; + \;\unitcpo_{bad} \quad
%%     \end{array}\]
%% The following continuous functions also exist, each being the inverse of the 
%% other (i.e. composing to the identity function on the corresponding cpo):
%% \[\begin{array}{l}
%%   \roll : (V_{\infty})_\bot =>_c D_{\infty} \\ 
%%   \unroll : D_{\infty} =>_c (V_{\infty})_\bot
%% \end{array}\] 
%% \end{lemma}


%% \paragraph{Definability of application}
%% We may now {\em define} application $\dapp : D_\infty \times D_\infty =>_c D_\infty$ as a continuous
%% function: 
%% {\setlength{\arraycolsep}{2pt}
%% \[\begin{array}{rcll}
%%    \dapp & = & \multicolumn{2}{l}{\dlambda d @.@ \roll(\bind_g (\unroll (\pi_1(d))))} \\
%%    \text{ where } g & = &  [ & \bot : \prod_{\ar_1}{D_\infty} =>_c D_\infty =>_c (V_\infty)_\bot \\
%%                     &   &  , & \ldots \\
%%                     &   &  , & \bot : \prod_{\ar_k}{D_\infty} =>_c D_\infty =>_c (V_\infty)_\bot \\
%%                     &   &  , & \dlambda d' @.@ \unroll(d'(\pi_2(d))) \\
%%                     &   &  , & \dlambda b @.@ \dlambda d. \ret(\inj{bad}(b))\hspace{2pt} ] 
%% \end{array}\]}%
%% Informally, what does the $\dapp$ combinator express? If the first
%% argument is $\bot$ it will return $\bot$, if the first argument is an
%% injection $\inj{bad}$ it will return the same, if it corresponds to a
%% data constructor it will return $\bot$. Finally, if the first element
%% is indeed a function, it will apply it.
%% \spj{Why not use $\dapp(fun, arg) = ...$ rather than all this stuff with angle brackets and pis. 
%% That just adds clutter.}
%% As a notational convention, we have used notation $\langle , \rangle$ to introduce pairs and $[\ldots]$ to 
%% eliminate ($n$-ary) sums. 
%% \spj{I don't understand the $[\ldots]$ notation.}
%% The projections $\pi_1$ and $\pi_2$ are the obvious continuous projections from the 
%% binary product space of $D_{\infty}$. We use notation $\inj{K}$ to denote the continuous map that injects some $n$-ary product of $D_{\infty}$ 
%% corresponding to the arity of constructor $K$ into the sum $V_{\infty}$.
%% We use notation $\inj{->}$ to denote the continuous injection of $(D_{\infty} =>_c D_{\infty})$ into $V_{\infty}$ and finally, 
%% $\inj{bad}$ for the unit injection into $V_{\infty}$. We use ordinary application notation 
%% $d(d')$ \spj{instead of $app(d,d')$?} \dv{No, instead of the @apply@ combinator of the cpo $D_\infty =>_c D_\infty$, which is different
%% than $\dapp$ which has a completely different signature than @apply@ (its arguments are both elements of $D_\infty$)} when $d : D_\infty =>_c D_\infty$ and
%% $d'$ is an element of $D_\infty$. \spj{Why inconsistent notation? Why not say $d':D_{\infty}$?}
%% Indeed application, partial application, and currying are all definable in cpos of continuous functions, 
%% so we will be using $\lambda$-calculus notation for our domain theory, as above. 


%% \paragraph{Denotational semantics of expressions and programs}
%% =======
\subsection{Contracts}


\begin{figure}
\[\begin{array}{lcl}
e \in \{ x\;\mid\;e_p\} & <=> &  e \not\Downarrow \text{ or } e_p[e/x] \not\Downarrow \text{ or} \\ 
                        &     &  e_p[e/x] \Downarrow True \\
e \in (x{:}\Ct_1) -> \Ct_2 & <=> & 
                        \forall e' \in \Ct_1.\; (e\;e') \in \Ct_2[e'/x] \\
e \in \Ct_1 \& \Ct_2 & <=> & e \in \Ct_1 \text{ and } e \in \Ct_2 \\
e \in \CF            & <=> & \forall {\cal C}. BAD \not\in {\cal C} \Rightarrow e \not\Downarrow BAD
\end{array} 
\]
\caption{Operational semantics of contracts} \label{f:contract-spec-op}
\end{figure}

We now turn our attention to contracts. The syntax of contracts
is given in Figure~\ref{fig:syntax} and includes base contracts
$\{ x \mid e \}$, arrow contructs $(x : \Ct_1) -> \Ct_2$, conjunctions
$\Ct_1 \& \Ct_2$ and crash-freedom $\CF$. Previous work~\cite{xu+:contracts} 
includes other constructs as well, but these constructs are enough to verify 
many programs and already demonstrate the interesting theoretical and practical problems.

We write $e \in C$ to mean ``the expression $e$ satisfies the contract $C$'', and similarly
for functions $f$.
Figure~\ref{f:contract-spec-op} says what it means to say $e \in C$,
based closely on earlier work \cite{xu}.  The specification is
simple, declarative, and intended to be comprehensible to programmers.
For example, an expression $e$ satisfies the function contract $\Ct_1 \& \Ct_2$ if and 
only if $(e e')$ satisfies $\Ct_2$ whenever $e'$ satisfies $Ct_1$.

Crucially, base contracts $\{x|e\}$ allow arbitrary $\cal L$
expressions (in our implementation, arbitrary Haskell expressions),
rather than being restricted to some well behaved meta-language.  This
is great for the programmer because the language and its library
functions is familiar, but it poses a challenge for verification
because these expressions in contracts may themselves diverge or
crash.


% -----------------------------------------------------------------
\section{Translating $\cal L$ to first-order logic} \label{ssect:denot-fol}

Our goal is to answer the question ``does expression $e$ satisfy
contract $C$?''.  Our plan is to translate both the expression and the
contract into first-order logic (FOL), and get a standard FOL prover
to do the heavy lifting.  

\spj{Is this next paragraph right?  I want to set the scene.}
What does it mean to translate an expression to first-order logic?
We are primarily interested in reasoning about equality, so we might
hope for this (informally):
$$
e_1 =_S e_2 \;<=>\; \etrans{}{}{e_1} =_{FOL} \etrans{}{}{e_2}
$$
where $=_S$ means semantic equality, $=_{FOL}$ means equality in first-order logic,
and $\etrans{}{}{e}$ is the translation of $e$ to a FOL term. That is, we can 
reason about the equality of Haskell terms by translating them into FOL using
a FOL theorem prover. 

In this section we formalise our new translation, and describe how we use it to
verify contracts.

\subsection{The FOL language}

\begin{figure}
\[\begin{array}{c} 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Terms}} \\
  s,t & ::=  & x                          & \text{Variables} \\ 
      & \mid & f(\ol{t})                  & \text{Function applications} \\
      & \mid & K(\ol{t})                  & \text{Constructor applications} \\ 
      & \mid & \sel{K}{i}(t)              & \text{Constructor selectors} \\ 
      & \mid & f_{ptr} \mid app(t,s)       & \text{Pointers and application} \\
      & \mid & \unr \mid \bad             & \text{Unreachable, bad} \\ \\
\multicolumn{3}{l}{\text{Formulae}} \\ 
 \phi & ::=  & \lcf{t}    & \text{Crash-freedom} \\
%%      & \mid & \lncf{t}   & \text{Can provably cause crash} \\
      & \mid & t_1 = t_2  & \text{Equality} \\ 
      & \mid & \phi \land \phi \mid \phi \lor \phi \mid \neg \phi \\
      & \mid & \forall x @.@ \phi \mid \exists x @.@ \phi \\ \\ 
\multicolumn{3}{l}{\text{Abbreviation}} \\ 
\multicolumn{4}{l}{app(t,\oln{s}{n}) = (\ldots(app(t,s_1),\ldots s_n)\ldots)}
\end{array}
\end{array}\]
\caption{Syntax of FOL}\label{fig:fol-image}
\end{figure}

We begin with the syntax of the FOL language, which
is given in Figure~\ref{fig:fol-image}. Terms include function 
applications $f(\ol{t})$, constructor applications $K(\ol{t})$, variables. They 
also include, for each data constructor $K^\ar$ in the signature $\Sigma$ with 
arity $\ar$ a set of {\em selector functions} $\sel{K}{i}(t)$ for $i \in 1 \ldots \ar$.
Not all functions are fully applied, so to accommodate partial applications, we use 
the term $app(t,s)$. Functions that are not applied, but rather passed on as essentially 
function pointers will be represented as variables (or nullary functions) $f_{ptr}$. Finally 
we introduce two new syntactic constructs $\unr$ and $\bad$. As an abreviation we often use
$app(t,\ol{s})$ for the sequence of applications to each $s_i$, as 
Figure~\ref{fig:fol-image} shows.

Figure~\ref{fig:fol-image}
also includes the syntax of formulae that will appear in our translation.
It is just first-order logic with equality, plus a predicate $\lcf{t}$ for 
crash-freedom, whose semantics we discuss in Section~\ref{sect:contracts}.


\begin{figure}\small
\setlength{\arraycolsep}{2pt} 
\[\begin{array}{c} 
\ruleform{\etrans{\Sigma}{\Gamma}{e} = \formula{t} } \\ \\
\begin{array}{rcl}
\etrans{\Sigma}{\Gamma}{x} & = & \formula{x} \\
\etrans{\Sigma}{\Gamma}{f[\ol{\tau}]} & = & \formula{f_{ptr}} \\
\etrans{\Sigma}{\Gamma}{K[\ol{\tau}](\ol{e})} & = & \formula{K(\ol{\etrans{e}{\Gamma}{t}})} \\
\etrans{\Sigma}{\Gamma}{e_1\;e_2} & = & \formula{app(\etrans{\Sigma}{t_1}{\Gamma},
                                                     \etrans{\Sigma}{t_1}{\Gamma})} \\
\etrans{\Sigma}{\Gamma}{@BAD@} & = & \formula{\bad}
\end{array}
\\ \\
\ruleform{\dtrans{\Sigma}{d} = \formula{\phi}} \\ \\
\begin{array}{rcl}
\dtrans{\Sigma}{f \;\ol{a}\;\ol{(x{:}\tau)} = e} 
  & = & \formula{f(\ol{x}) = \etrans{\Sigma}{\Gamma}{e}} \\
\multicolumn{3}{l}{\dtrans{\Sigma}
    {f \;\ol{a}\;\ol{(x{:}\tau)} = @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}}} \\
\multicolumn{3}{l}{
\quad 
  \begin{array}[t]{rl}
    = & \formula{(t = \bad => s = bad)} \\ 
    \land & \formula{(\forall \ol{y} @.@ t = K_1(\ol{y}) => s = t_{K_1})\;\land \ldots}  \\
    \land & \formula{(t{\neq}\bad\;\land\;
                 t{\neq}K_1(\oln{{\sel{K_1}{i}}(t)}{})\;\land\;\ldots) => s{=}\unr)} \\
    \mbox{where} &  
         \begin{array}[t]{rcl} 
            s & = & f(\ol{x}) \\  
            t & = & \etrans{\Sigma}{\Gamma}{e} 
         \end{array}
 \end{array}
}
\end{array}
\\ \\
\ruleform{\ptrans{\Sigma}{P} = \formula{\phi} } \quad
\ptrans{\Sigma}{\ol{d}} = \ol{\dtrans{\Sigma}{d}}
% \ruleform{\Dtrans{\Sigma}{\Gamma}{u} = \formula{\phi} } \\ \\
% 
% \prooftree
%   \begin{array}{c}
%   (f{:}\forall\oln{a}{n} @.@ \tau) \in \Sigma
%   \end{array}
%   --------------------------------------{TFVar}
%   \etrans{\Sigma}{\Gamma}{f[\oln{\tau}{n}]} = \formula{f_{ptr}}
%   ~~~~ 
%   x \in \Gamma 
%   --------------------------------------{TVar}
%   \etrans{\Sigma}{\Gamma}{x} = \formula{x}
%   ~~~~~ 
%   \begin{array}{c}
%   (K{:}\forall\oln{a}{n} @.@ \ol{\tau} -> T\;\as) \in \Sigma \quad
%   \ol{\etrans{\Sigma}{\Gamma}{e} = \formula{t}}
%   \end{array}
%   --------------------------------------{TCon}
%   \etrans{\Sigma}{\Gamma}{K[\oln{\tau}{n}](\ol{e})} = \formula{K(\ol{t})}
%   ~~~~~
%   \phantom{\Gamma}
%   --------------------------------------{TBad}
%   \etrans{\Sigma}{\Gamma}{@BAD@} = \formula{\bad}
%   ~~~~
%   \begin{array}{c}
%   \etrans{\Sigma}{\Gamma}{e_1} = \formula{t_1} \\
%   \etrans{\Sigma}{\Gamma}{e_2} = \formula{t_2}
%   \end{array}
%   --------------------------------------{TApp}
%   \etrans{\Sigma}{\Gamma}{e_1\;e_2} = \formula{app(t_1,t_2)}
% \endprooftree \\ \\ 
% \ruleform{\utrans{\Sigma}{\Gamma}{f\;\ol{a}\;\ol{x{:}\tau} = u} = \formula{\phi}} \\ \\ 
% \\
% \prooftree
%    \etrans{\Sigma}{\Gamma}{e} = \formula{t}
%    ----------------------------------------{DExp}
%    \utrans{\Sigma}{\Gamma}{f\;\ol{a}\;\ol{x{:}\tau} = e} = \formula{f(\ol{x}) = t} 
%    ~~~~~
%   \begin{array}{l}
%   \etrans{\Sigma}{\Gamma}{e} = \formula{t} \\
% %%  constrs(\Sigma,T) = \ol{K} \\
%   \text{for each branch}\;(K\;\oln{y}{l} -> e') \text{ it is } \etrans{\Sigma}{\Gamma,\ol{y}}{e'} = \formula{ t_K }
% %%  \begin{array}{l}
% %% %%           (K{:}\forall \cs @.@ \oln{\sigma}{l} -> T\;\oln{c}{k}) \in \Sigma \text{ and }
% %%            \etrans{\Sigma}{\Gamma,\ol{y}}{e'} = \formula{ t_K }
% %%   \end{array}
%   \end{array}
%   ------------------------------------------{DCase}
%   {  \setlength{\arraycolsep}{2pt} 
%   \begin{array}{l}
%   \utrans{\Sigma}{\Gamma}{s \sim @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}} = \\
%   \;\;\formula{ \begin{array}{l} 
%           (t = \bad => s = bad)\;\land \\ 
%           (\forall \ol{y} @.@ t = K_1(\ol{y}) => s = t_{K_1}\;\land \ldots \land \\
%           (t{\neq}\bad\;\land\;t{\neq}K_1(\oln{{\sel{K_1}{i}}(t)}{})\;\land\;\ldots => s{=}\unr) 
% %% (t = \bad /\ s = \bad)\;\lor\;(s = \unr)\;\lor \\
% %%                                 \quad      \bigvee(t = K(\oln{{\sel{K}{i}}(t)}{}) \land
% %%                                            s = t_K[\oln{\sel{K}{i}(t)}{}/\ol{y}])
%                    \end{array}
%            }
%   \end{array}}
% \endprooftree \\ \\ 
% \ruleform{ \Dtrans{\Sigma}{P} = \formula{\phi}} \\ \\ 
% \prooftree
%      \begin{array}{l}       
%        \text{for each} (f\;\ol{a}\;\ol{x{:}\tau} = u \in P \\ 
%           \quad \utrans{\Sigma}{\ol{x}}{f(\ol{x}) \sim u} = \formula{\phi}
%      \end{array}
%      --------------------{TDefs}
%      \Dtrans{\Sigma}{P} = \bigwedge_{P} \formula{\forall \ol{x} @.@ \phi}
% \endprooftree 
\end{array}\]
\caption{Translation of programs to logic}\label{fig:etrans}
\end{figure}

\subsection{Translation of expressions to FOL}

The translation of expressions $e$ which may contain term variables from $\Gamma$ to 
first-order logic terms is given in Figure~\ref{fig:etrans} with the function
$\etrans{}{\Gamma}{e}$. Variables map to variables, functions map to function pointers, 
applications map to applications of $app(\cdot,\cdot)$, constructors map to constructor
function applications and @BAD@ maps to $\bad$.

A program $P$ is translated to a first order logic formula with the function 
$\dtrans{}{P}$. For each function definition of a function $f^\ar$ we generate
a quantified formula $\forall \xs @.@ \utrans{}{\ol{x}}{f(\ol{x}) \sim u}$ where $u$ is the 
right-hand side of the definition of $f$ in $P$. The function 
$\utrans{}{\Gamma}{t \sim u}$ generates the necessary formula to equate the left-hand-side
first-order term $t$ to a term corresponding to $u$. The context $\Gamma$ is present
for book-keeping purposes. As Figure~\ref{fig:etrans} shows there are two cases to 
consider. If $u$ is just a @case@-free expression $e$, in which case we generate
an equality formula (rule \rulename{DExp}. 

\subsection{Data types and {\tt case} expressions}

The most interesting rule is \rulename{DCase}. If the translation of the scrutinee
$t$ is equal to $\bad$ then the left-hand-side $s$ must be equal to $\bad$. If it is 
equal to some constructor application from the case expressions then the right 
hand-side is equal to the corresponding translation $t_K$ of some pattern branch. Finally
if the scrutinee is not $\bad$ nor some of the constructors in the branch we assert that
the left-hand-side $s$ must be equal to $\unr$. The rule precisely corresponds to our 
intuitions from the denotational semantics: in the denotational semantics, whenever
we attempt to scrutinize a constructor of a different type in a case expression, the 
resulting denotation is simply $\bot$. This reinforces our desire to treat type errors
as $\bot$ values. 

\subsection{Higher order functions}

\spj{Here I want us to describe how app works.}

\subsection{Translation of contracts to FOL}

\begin{figure}\small
\[\begin{array}{c} 
\ruleform{\ctrans{\Sigma}{\Gamma}{e \in \Ct} = \formula{\phi}} \\ \\ 
\prooftree
  \begin{array}{c}
   \etrans{\Sigma}{\Gamma}{e} = \formula{t} \quad
   \etrans{\Sigma}{\Gamma,x}{e'} = \formula{t'}
  \end{array}
  ------------------------------------------{CTransBase}
  \begin{array}{l}
   \ctrans{\Sigma}{\Gamma}{e \in \{(x{:}\tau) \mid e' \}} = \\
  %% \Sigma;\Gamma |- e \in \{(x{:}\tau \mid e' \}
  \;\;\formula{(t{=}\unr) \lor (t'[t/x]{=}\unr) \lor (t'[t/x]{=}\True)}
  \end{array}
  ~~~~~ 
  \begin{array}{c}
  \ctrans{\Sigma}{\Gamma,x}{x \in \Ct_1} {=} \formula{\phi_1} \quad
  \ctrans{\Sigma}{\Gamma,x}{e\;x \in \Ct_2} {=} \formula{\phi_2}
  \end{array} 
  ------------------------------------------{CTransArr}
  \begin{array}{l} 
  \ctrans{\Sigma}{\Gamma}{e \in (x{:}\Ct_1) -> \Ct_2} = 
  \formula{\forall x @.@ \neg \phi_1 \lor \phi_2} 
  \end{array}
  ~~~~~
  \begin{array}{c}
  \ctrans{\Sigma}{\Gamma}{e \in \Ct_1} = \formula{ \phi_1} \quad
  \ctrans{\Sigma}{\Gamma}{e \in \Ct_2} = \formula{ \phi_2}
  \end{array}
  ------------------------------------------{CTransConj}
  \ctrans{\Sigma}{\Gamma}{e \in \Ct_1 \& \Ct_2} = \formula{ \phi_1 /\ \phi_2}
  ~~~~~
  \etrans{\Sigma}{\Gamma}{e} =  \formula{t}
  -------------------------------------------{CTransCf}
  \ctrans{\Sigma}{\Gamma}{e \in \CF} = \formula{\lcf{t}}
 \endprooftree \\ \\ 
\ruleform{\Th_\lcfZ} \\ \\ 
\begin{array}{lll} 
 \textsc{AxCfBU} & \formula{\lcf{\unr} /\ \lncf{\bad}} \\
 \textsc{AxCfC}  & \formula{\forall \oln{x}{n} @.@ \lcf{K(\ol{x})} <=> \bigwedge\lcf{\ol{x}}} \\
                 & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma
\end{array}
\end{array}\]
\caption{Translation of contracts to logic}\label{fig:contracts-minless}
\end{figure}

We now turn to expressing contract semantics in first-order logic. 
We do not translate a contract; rather we translate the claim $e \in C$.
Once we have translated $e \in \Ct$ to a first-order logic formula, we can
ask the prover to prove it; and, if successful, we can claim that indeed
$e$ does satisfy $C$.  Of course that needs proof, which we address in Section~\ref{xxx}.

Figure~\ref{fig:contracts-minless} presents the details of the 
function $\ctrans{}{\Gamma}{e \in \Ct}$.  We also include 
an axiomatization of crash-freedom with the axiom groups \rulename{AxCfBU} and 
\rulename{AxCfC}. The first axiom asserts that $\unr$ is crash-free and $\bad$ is not, 
whereas the second axiom asserts that a constructor value is crash-free if and only iff 
all of its children nodes are crash-free. 

\subsection{Proving that a function satisfies a contract}

\spj{How to actually prove that a function satisfies a contract.  Or maybe
it's enough to forward-reference the implementation section}
