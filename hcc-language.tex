To formalise the ideas behind our implementation, we define a
tiny source language $\theLang$:
a polymorphic, higher-order, call-by-name $\lambda$-calculus with
algebraic datatypes, pattern matching, and recursion.
Our actual implementation treats all of Haskell, by using GHC as a front
end to translate Haskell into language $\theLang$.

\kc{I think we should add an example here, of a Haskell program and its translation into FOL. Suggestion: map.}

\subsection{Syntax of $\theLang$} \label{s:syntax}

\begin{figure}
\[\begin{array}{l}
\begin{array}{lrll}
\multicolumn{4}{l}{\text{Programs, definitions, and expressions}} \\
P   & ::= & d_1 \ldots d_n \\
d   & ::= & f\; \ol{a} \; \ol{(x\!:\!\tau)} = u \\
u   & ::= & \multicolumn{2}{l}{e \; \mid \; @case@\;e\;@of@\;\ol{K\;\ol{y} -> e}} \\
e  & ::=  & x            & \text{Variables} \\
   & \mid & f[\ol{\tau}] & \text{Function variables} \\
   & \mid & K[\ol{\tau}](\ol{e}) & \text{Data constructors} \\
   & \mid & e\;e         & \text{Applications} \\
   & \mid & @BAD@        & \text{Runtime error} \\
\end{array}\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Syntax of closed values}} \\
 v,w & ::= & K^\ar[\ol{\tau}](\oln{e}{\ar}) \;\mid\; f^\ar[\ol{\tau}]\;\oln{e}{m < \ar} \;\mid\; @BAD@ \\ \\
\end{array}
\\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Contracts}} \\
 \Ct & ::=  & \{ x \mid e \}        & \text{Base contracts}  \\
     & \mid &  (x : \Ct_1) -> \Ct_2      & \text{Arrow contracts} \\
     & \mid & \Ct_1 \& \Ct_2             & \text{Conjunctions}   \\
     & \mid & \CF                        & \text{Crash-freedom}   \\
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Types}} \\
\tau,\sigma & ::=  & T\;\taus & \text{Datatypes} \\
            & \mid & a \mid \tau -> \tau
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Type environments and signatures}} \\
\Gamma & ::=  & \cdot \mid \Gamma,x \\
\Delta & ::=  & \cdot \mid \Delta,a \mid \Delta,x{:}\tau \\
\Sigma & ::=  & \cdot \mid \Sigma,T{:}n \mid \Sigma,f{:}\forall\ol{a} @.@ \tau \mid \Sigma,K^{\ar}{:}\forall\ol{a} @.@ \oln{\tau}{\ar} -> T\;\as
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Auxiliary functions}} \\
%% constrs(\Sigma,T) & = & \{ K \mid (K{:}\forall \as @.@ \taus -> T\;\as) \in \Sigma \} \\
(\cdot)^{-}            & = & \cdot \\
(\Delta,a)^{-}         & = & \Delta^{-} \\
(\Delta,(x{:}\tau))^{-} & = & \Delta^{-},x
%% \tyar{D}{f} & = & n & \\
%%             & \multicolumn{3}{l}{\text{when}\; (f |-> \Lambda\oln{a}{n} @.@ \lambda\ol{x{:}\tau} @.@ u) \in D} \\
%% \tmar{D}{f} & = & n & \\
%%             & \multicolumn{3}{l}{\text{when}\; (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{n} @.@ u) \in D}
\end{array}
\end{array}\]
\caption{Syntax of $\theLang$ and its contracts}\label{fig:syntax}
\end{figure}

Figure~\ref{fig:syntax} presents the syntax of $\theLang$.  A program
$P$ consists of a set of recursive function definitions $d_1 \ldots
d_n$. Each definition has a left hand side that binds its type-variable and
term-variable parameters;
if $f$ has $n$ term-variable parameters we say that
it has arity $n$, and sometimes write it $f^n$.
The right hand side $u$ of a definition is either a @case@ expression or a
@case@-free expression $e$.  A @case@-free expression consists of
variables $x$, function variables $f[\taus]$ fully applied to their
type arguments, applications $e_1\;e_2$, and data constructor applications
$K[\taus](\ol{e})$. The latter return values of datatypes $T\;\taus$. As a 
notation, we use $\oln{x}{n}$ for sequences of elements of size $n$. When $n$ is
omitted $\ol{x}$ has a size which is implied by the context or is not
interesting.

A program \emph{crashes} if it evaluates the special value @BAD@.
For example, we assume that the standard Haskell function @error@
simply invokes @BAD@, thus:
\begin{code}
  error :: String -> a
  error s = BAD
\end{code}
Moreover, we assume that all incomplete pattern-matches are completed, with the
missing case yielding @BAD@.  For example:
\begin{code}
  head :: [a] -> [a]
  head (x:xs) = x
  head []     = BAD
\end{code}
In our context, @BAD@ is our way to saying what it means for a program to ``go wrong'',
and verification amounts to proving that a program cannot invoke @BAD@.

Our language embodies several convenient syntactic constraints:
(i)~$\lambda$-abstractions occur only at the top-level,
(ii)~@case@-expressions can only immediately follow a function
definition, and (iii) constructors are fully applied.
Any Haskell program can be transformed into this restricted
form by lambda-lifting, @case@-lifting, and eta-expansion respectively,
and our working prototype does just this.
However this simpler language is notationally
convenient for the translation of programs to first-order logic.

$\theLang$ is an explicitly-typed language, and we assume the existence
of a typing relation $\Sigma |- P$, which checks that a program
conforms to the definitions in the signature $\Sigma$. A signature
$\Sigma$ (Figure~\ref{fig:syntax}) records the declared data types,
data constructors and types of functions in the program $P$. The
well-formedness of expressions is checked with a typing relation
$\Sigma;\Delta |- u : \tau$, where $\Delta$ is a typing environment,
also in Figure~\ref{fig:syntax}.  We do not give the details of the
typing relation since it is standard.
Our technical development and analysis in the following sections
assume that the program has been checked for type errors.
The typing judgement should check that all pattern matches are
exhaustive; as mentioned above, missing cases should return @BAD@.

The syntax of closed values is also given in Figure~\ref{fig:syntax}. Since we do not
have arbitrary $\lambda$-abstractions, values can only be partial function applications
$f^\ar[\ol{\tau}]\;\oln{e}{m < \ar}$, data constructor applications $K[\tau](\ol{e})$,
and the error term @BAD@.

The operational semantics of $\theLang$ is entirely standard, and we do not give it here.
We write $P |- u \Downarrow v$ to mean ``in program P, right-hand side $u$ evaluates to
value $v$'',

% -------------------- Omitted section on operational semantics ------------
% \subsection{Operational semantics of $\theLang$}
%
% \spj{Why do we give an operational as well as denotational semantics?}
% The big-step operational semantics of our language is given in
% Figure~\ref{fig:opsem}, which contains no surprises. One interesting
% detail of big-step semantics is that they do not distinguish between non-termination
% and ``getting stuck'', meaning that if $P \not|- e \Downarrow v$ then $e$ could either diverge or its
% evaluation could get stuck. We return to this convenient for our purposes form of operational
% semantics later. \spj{Where later? Also this sentence is hard to parse; indeed I'm not quite
% sure what it means.}
% \begin{figure}
% \[\begin{array}{c}
% \ruleform{P |- u \Downarrow v} \\
% \prooftree
% \begin{array}{c} \ \\
% \end{array}
% -------------------------------------{EVal}
% P |- v \Downarrow v
% ~~~~~
% \begin{array}{c}
% (f \;\ol{a}\;\oln{(x{:}\tau)}{m} = u) \in P \\
% P |- u[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \Downarrow v
% \end{array}
% -------------------------------------{EFun}
% P |- f[\ol{\tau}]\;\oln{e}{m} \Downarrow v
% ~~~~~
% \begin{array}{c}
% P |- e_1 \Downarrow v_1 \\
% P |- v_1\;e_2 \Downarrow w
% \end{array}
% ------------------------------------------------{EApp}
% P |- e_1\;e_2 \Downarrow w
% ~~~~~
% \begin{array}{c}
% P |- e_1 \Downarrow @BAD@
% \end{array}
% ------------------------------------------------{EBadApp}
% P |- e_1\;e_2 \Downarrow @BAD@
% ~~~~~
% %% \endprooftree \\ \\
% %% \ruleform{P |- u \Downarrow v} \\ \\
% %% \prooftree
% %% P |- e \Downarrow v
% %% -------------------------------------{EUTm}
% %% P |- e \Downarrow v
% %% ~~~~
% \begin{array}{c}
% P |- e \Downarrow K_i[\ol{\sigma}_i](\ol{e}_i) \quad
% P |- e'_i[\ol{e}_i/\ol{y}_i] \Downarrow w
% \end{array}
% ------------------------------------{ECase}
% P |- @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'} \Downarrow w
% ~~~~~
% \begin{array}{c}
% P |- e \Downarrow @BAD@ \\
% \end{array}
% ------------------------------------{EBadCase}
% P |- @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'} \Downarrow @BAD@
% %% \begin{array}{c}
% %% (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{m} @.@ @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}) \in D \\
% %% D |- e[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \Downarrow @BAD@ \\
% %% \end{array}
% %% -------------------------------------{EBadCase}
% %% D |- f[\ol{\tau}]\;\oln{e}{m} \Downarrow @BAD@
% \endprooftree
% \end{array}\]
% \caption{Operational semantics of $\theLang$}\label{fig:opsem}
% \end{figure}
% %% We can state some standard properties of the typing and evaluation relation.
% %% \begin{lemma}[Subject reduction]
% %% Assume $\Sigma |- P$ and $\Sigma;\cdot |- e : \tau$
% %% If $P |- e \Downarrow w$ then $P |- value(w)$ and $\Sigma;\cdot |- w : \tau$.
% %% \end{lemma}
% The operational semantics of Figure~\ref{fig:opsem} has the possibility of non-determinism because
% of the overlapping of several rules for applications. But that is not a problem, as we can prove that evaluation
% is deterministic using the following two lemmas.
% \begin{lemma}[Value determinacy]
% If $\Sigma;\cdot |- v : \tau$ and
% $\Sigma |- P$ and $P |- v \Downarrow w$ then $ v = w $.
% \end{lemma}
% \begin{lemma}[Determinacy of evaluation]
% If $\Sigma;\cdot |- e : \tau$ and
% $\Sigma |- P$ and $\Sigma;\cdot |- e \Downarrow v_1$ and $\Sigma;\cdot |- e \Downarrow v_2$ then
% $v_1 = v_2$.
% \end{lemma}
% Finally, big-step soundness asserts that an expression that evaluates results in a
% well-typed value.
% \begin{lemma}[Big-step soundness]
% If $\Sigma;\cdot |- e : \tau$ and
% $\Sigma |- P$ and $\Sigma;\cdot |- e \Downarrow v$ then $\Sigma;\cdot |- v : \tau$.
% \end{lemma}

\subsection{Contracts}

% \begin{figure}
% \[\begin{array}{lcl}
% e \in \{ x \mid e_p\} & <=> &  e \not\Downarrow \text{ or } e_p[e/x] \not\Downarrow \text{ or} \\
%                         &     &  e_p[e/x] \Downarrow True \\
% e \in (x{:}\Ct_1) -> \Ct_2 & <=> &
%                         \forall e' \in \Ct_1.\; (e\;e') \in \Ct_2[e'/x] \\
% e \in \Ct_1 \& \Ct_2 & <=> & e \in \Ct_1 \text{ and } e \in \Ct_2 \\
% e \in \CF            & <=> & \forall {\cal C}. BAD \not\in {\cal C} \Rightarrow e \not\Downarrow BAD
% \end{array}
% \]
% \caption{Operational semantics of contracts} \label{f:contract-spec-op}
% \end{figure}

We now turn our attention to contracts. The syntax of contracts
is given in Figure~\ref{fig:syntax} and includes base contracts
$\{ x \mid e \}$, arrow contracts $(x : \Ct_1) -> \Ct_2$, conjunctions
$\Ct_1 \& \Ct_2$ and crash-freedom $\CF$. Previous work~\cite{xu+:contracts}
includes other constructs as well, but the constructs we give here are enough to verify
many programs and exhibit the interesting theoretical and practical problems.

We write $e \in C$ to mean ``the expression $e$ satisfies the contract $C$'', and similarly
for functions $f$.  We will say what contracts mean formally in
Section~\ref{s:den-sem-contracts}.  However, here is their informal meaning:
\begin{itemize*}
\item $e \in \{x | e'\}$ means that $e$ does not evaluate to a value
  or $e'[e/x]$ evaluates to @True@ or does not evaluate to a value.
Notice that $e'$ is an arbitrary expression
(in our implementation, arbitrary Haskell expressions),
rather than being restricted to some well behaved meta-language.  This
is great for the programmer because the language and its library
functions are familiar, but poses a challenge for verification
because these expressions in contracts may themselves diverge or
crash.
\item $e \in (x:\Ct_1) \rightarrow \Ct_2$ means that whenever $e'$ satisfies $\Ct_1$, it
is the case that $(e\,e')$ satisfies $\Ct_2[e'/x]$.
\item $e \in \Ct_1 \& \Ct_2$ means that $e$ satisfies both $\Ct_1$ and $\Ct_2$.
\item $e \in \CF$ means that $e$ is \emph{crash free}; that is $e$ does not
crash regardless of what context it is plugged into (see Section~\ref{s:cf-fol}).
\end{itemize*}

% -----------------------------------------------------------------
\section{Translating $\theLang$ to first-order logic} \label{ssect:trans-fol}

Our goal is to answer the question ``does expression $e$ satisfy
contract $C$?''.  Our plan is to translate the expression and the
contract into a first-order logic (FOL) term and formula respectively, and get an 
FOL prover to do the heavy lifting. In this section we formalise our translation, 
and describe how we use it to verify contracts.

\subsection{The FOL language}

\begin{figure}
\[\begin{array}{c}
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Terms}} \\
  s,t & ::=  & x                          & \text{Variables} \\
      & \mid & f(\ol{t})                  & \text{Function applications} \\
      & \mid & K(\ol{t})                  & \text{Constructor applications} \\
      & \mid & \sel{K}{i}(t)              & \text{Constructor selectors} \\
      & \mid & f_{ptr} \mid app(t,s)       & \text{Pointers and application} \\
      & \mid & \unr \mid \bad             & \text{Unreachable, bad} \\ \\
\multicolumn{3}{l}{\text{Formulae}} \\
 \phi & ::=  & \lcf{t}    & \text{Crash-freedom} \\
%%      & \mid & \lncf{t}   & \text{Can provably cause crash} \\
      & \mid & t_1 = t_2  & \text{Equality} \\
      & \mid & \phi \land \phi \mid \phi \lor \phi \mid \neg \phi \\
      & \mid & \forall x @.@ \phi \mid \exists x @.@ \phi \\ \\
\end{array}
\\
\multicolumn{1}{l}{\text{Abbreviations}} \\
\begin{array}{rcl}
app(t,\oln{s}{n}) & = & (\ldots(app(t,s_1),\ldots s_n)\ldots) \\
\phi_1 \Rightarrow \phi_2 & = & \neg \phi_1 \lor \phi_2
\end{array}
\end{array}\]
\caption{Syntax of FOL}\label{fig:fol-image}
\end{figure}

We begin with the syntax of the FOL language, which is given in
Figure~\ref{fig:fol-image}. There are two syntactic forms,
\emph{terms} and \emph{formulae}. Terms include saturated function applications
$f(\ol{t})$, that is, fully applied to all the arguments that were declared at their
definition. They also include saturated constructor applications $K(\ol{t})$, 
variables, and, for each data constructor $K^\ar$ in the signature
$\Sigma$ with arity $\ar$, a set of {\em selector functions}
$\sel{K}{i}(t)$ for $i \in 1 \ldots \ar$.  The terms $app(t,s)$ and
$f_{ptr}$ concern the higher-order aspects of $\theLang$
(namely un-saturated applications), which we
discuss in Section~\ref{s:hof}.  Finally we introduce two new
syntactic constructs $\unr$ and $\bad$. As an abbreviation we often use
$app(t,\ol{s})$ for the sequence of applications to each $s_i$, as
Figure~\ref{fig:fol-image} shows.

A formula $\phi$ in Figure~\ref{fig:fol-image} is just a FOL
formula, augmented with a predicate $\lcf{t}$ for crash-freedom, which
we discuss in Section~\ref{s:cf-fol}.

\subsection{Translation of expressions to FOL}\label{ssect:trans-exprs}

% ---------------------------------------------------
\begin{figure}\small
\setlength{\arraycolsep}{2pt}
\[\begin{array}{c}
\ruleform{\ptrans{\Sigma}{P} = \formula{\phi} } \quad
\ptrans{\Sigma}{\ol{d}} = \bigwedge \ol{\dtrans{\Sigma}{d}}
\\ \\
\ruleform{\dtrans{\Sigma}{d} = \formula{\phi}} \\ \\
\begin{array}{rcl}
  \dtrans{\Sigma}{f \;\ol{a}\;\ol{(x{:}\tau)} = u}
    & =     & \formula{\forall \ol{x} @.@\, \utrans{\sigma}{u}{f(\ol{x})}} \\
    & \land & \formula{\forall \ol{x} @.@\, f(\ol{x}) = app(f_{ptr},\xs)} \\
\end{array}
\\ \\
\ruleform{\utrans{\Sigma}{u}{s} = \formula{\phi}} \\ \\
\begin{array}{rcl}
\utrans{\Sigma}{e}{s}
  & = & \formula{(s = \etrans{\Sigma}{\Gamma}{e})} \\
\multicolumn{3}{l}{\utrans{\Sigma}
    {@case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}}{s}} \\
\multicolumn{3}{l}{
\quad
  \begin{array}[t]{rl}
    = & \formula{(t = \bad => s = bad)} \\
    \land & \formula{(\forall \ol{y} @.@ t = K_1(\ol{y}) => s = \etrans{\Sigma}{\Gamma}{e'_1})\;\land \ldots}  \\
    \land & \formula{(t{\neq}\bad\;\land\;
                 t{\neq}K_1(\oln{{\sel{K_1}{i}}(t)}{})\;\land\;\ldots => s=\unr)} \\
    \mbox{where} & t  =  \etrans{\Sigma}{\Gamma}{e}
 \end{array}
}
\end{array}
\\ \\
\ruleform{\etrans{\Sigma}{\Gamma}{e} = \formula{t} } \\ \\
\begin{array}{rcl}
\etrans{\Sigma}{\Gamma}{x} & = & \formula{x} \\
\etrans{\Sigma}{\Gamma}{f[\ol{\tau}]} & = & \formula{f_{ptr}} \\
\etrans{\Sigma}{\Gamma}{K[\ol{\tau}](\ol{e})} & = & \formula{K(\ol{\etrans{\Sigma}{\Gamma}{e}})} \\
\etrans{\Sigma}{\Gamma}{e_1\;e_2} & = & \formula{app(\etrans{\Sigma}{\Gamma}{e_1},
                                                     \etrans{\Sigma}{\Gamma}{e_2})} \\
\etrans{\Sigma}{\Gamma}{@BAD@} & = & \formula{\bad}
\end{array}
\\ \\
\ruleform{\ctrans{\Sigma}{\Gamma}{e \in \Ct} = \formula{\phi}} \\ \\
\begin{array}{rcl}
\ctrans{\Sigma}{\Gamma}{e \in \{x\;\mid\;e' \}}
  & = & \formula{t{=}\unr} \\
  & \lor & \formula{t'[t/x]{=}\unr} \\
  & \lor & \formula{t'[t/x]{=}\True} \\
  & \mbox{where} &
    \begin{array}[t]{rcl}
      t  & = & \etrans{\Sigma}{\Gamma}{e} \; \text{and} \; t' = \etrans{\Sigma}{\Gamma}{e'}
    \end{array}
\\
\ctrans{\Sigma}{\Gamma}{e \in (x{:}\Ct_1) -> \Ct_2}
  & = & \formula{\forall x @.@ \ctrans{\Sigma}{\Gamma,x}{x \in \Ct_1}
                          \Rightarrow \ctrans{\Sigma}{\Gamma,x}{e\;x \in \Ct_2}}
\\
\ctrans{\Sigma}{\Gamma}{e \in \Ct_1 \& \Ct_2}
   & = & \formula{ \ctrans{\Sigma}{\Gamma}{e \in \Ct_1} /\ \ctrans{\Sigma}{\Gamma}{e \in \Ct_2}}
\\
\ctrans{\Sigma}{\Gamma}{e \in \CF} & = & \formula{\lcf{\etrans{\Sigma}{\Gamma}{e}}}
\end{array}
\end{array}\]
\caption{Translation of programs and contracts to logic}
   \label{fig:etrans}\label{fig:contracts-minless}
\end{figure}
% ---------------------------------------------------
\begin{figure}\small
\setlength{\arraycolsep}{1pt}
\[\begin{array}{c}
\ruleform{\text{Theory}\,\Th} \\
\begin{array}{ll}
\multicolumn{2}{l}{\text{Axioms for $bad$ and $unr$}} \\
 \textsc{AxAppBad}  & \formula{\forall x @.@ app(\bad,x){=}\bad}  \\
 \textsc{AxAppUnr}  & \formula{\forall x @.@ app(\unr,x){=}\unr}    \\
 \textsc{AxDisjBU} & \formula{\bad \neq \unr}  \\
\\
\multicolumn{2}{l}{\mbox{Axioms for data constructors}} \\
 \textsc{AxDisjC} & \formula{\forall \oln{x}{n}\oln{y}{m} @.@ K(\ol{x}) \neq J(\ol{y})} \\
                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
                  & \text{ and } (J{:}\forall\as @.@ \oln{\tau}{m} -> S\;\as) \in \Sigma \\
                  & \text{ and } K \neq J \\
 \textsc{AxDisjCBU} & \formula{(\forall \oln{x}{n} @.@ K(\ol{x}) \neq \unr \; \land \; K(\ol{x}) \neq \bad)} \\
                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
 \textsc{AxInj}   & \formula{\forall \oln{y}{n} @.@ \sel{K}{i}(K(\ys)) = y_i} \\
                  & \text{for every } K^\ar \in \Sigma \text{ and } i \in 1..n \\
\\
\multicolumn{2}{l}{\mbox{Axioms for crash-freedom}} \\
 \textsc{AxCfC}  & \formula{\forall \oln{x}{n} @.@ \lcf{K(\ol{x})} <=> \bigwedge\lcf{\ol{x}}} \\
                 & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
 \textsc{AxCfBU} & \formula{\lcf{\unr} /\ \lncf{\bad}} \\
\end{array}
\end{array}\]
\caption{Theory $\Th$: axioms of the FOL constants}\label{fig:prelude} \label{fig:data-cons}
\end{figure}

% ---------------------------------------------------
What exactly does it mean to translate an expression to first-order logic?
We are primarily interested in reasoning about equality, so we might
hope for this informal guiding principle:

\begin{quote}
If we can prove\footnote{From an appropriate axiomatisation of the semantics of 
programs.} in FOL that $\etrans{}{}{e_1} = \etrans{}{}{e_2}$ then
$e_1$ and $e_2$ are semantically equivalent.
\end{quote}

where $\etrans{}{}{e}$ is the translation of $e$ to a FOL term. That is, we can
reason about the equality of Haskell terms by translating them into FOL, and then using
a FOL theorem prover. The formal statement of this property is Corollary~\ref{cor:guiding-principle}.

The translation of programs, definitions, and expressions to FOL
is given in Figure~\ref{fig:etrans}.
The function $\ptrans{}{P}$ translates a program to a conjunction of formulae,
one for each definition $d$, using $\dtrans{}{d}$ to translate
each definition.
The first clause in $\cal D$s right-hand side uses $\cal U$ to 
translate the right hand side $u$, and quantifies over the $\ol{x}$.  
We will deal with the second clause of $\cal D$
in Section~\ref{s:hof}.

Ignoring @case@ for now (which we discuss in Section~\ref{s:case-fol}),
the formula $\utrans{}{e}{f(\ol{x})}$
simply asserts the equality $f(\ol{x}) = \etrans{}{}{e}$.
That is, we use a new function $f$ in the logic for each function definition in the
program, and assert that any application of $f$ is equal to (the logical translation of)
$f$'s right hand side. Notice that we erase type arguments in the translation
since they do not affect the semantics. You might think that the translation 
$f(\ol{x}) = \etrans{}{}{e}$ is entirely obvious but, surprisingly, it is only correct 
because we are in a call-by-name setting. The same equation is problematic in a 
call-by-value setting -- an issue 
we return to towards the end of Section~\ref{s:soundness}.

Lastly $\etrans{}{}{e}$ deals with expressions.  We will deal with
functions and application shortly
(Section~\ref{s:hof}), but the other equations for $\etrans{}{}{e}$
are straightforward.  Notice that $\etrans{}{}{@BAD@} = bad$, and recall
that @BAD@ is the $\theLang$-term used for an inexhaustive @case@ or a call
to @error@.  It follows from our guiding principle
that for any $e$, if we manage to prove in FOL that $ \etrans{}{}{e} = bad $, 
then the source program $e$
must be semantically equivalent to @BAD@, meaning that it definitely
crashes.

\subsection{Translating higher-order functions} \label{s:hof}

If $\theLang$ was
a first-order language, the translation of function calls would be easy:
$$
\etrans{\Sigma}{\Gamma}{f[\ol{\tau}]\;\ol{e}} = \formula{f(\ol{\etrans{}{}{e}})} \\
$$
At first it might be surprising that we can also translate a \emph{higher-order} language
$\theLang$ into first order logic, but in fact it is easy to do so, as
Figure~\ref{fig:etrans} shows.  We introduce into the logic
(a) a single new function $app$, and (b) a nullary constant $f_{ptr}$ for each function $f$
(see Figure~\ref{fig:fol-image}).
Then, the equations for $\etrans{}{}{e}$ translate application in $\theLang$ to
a use of $app$ in FOL, and any mention of function $f$ in $\theLang$ to a use
of $f_{ptr}$ in the logic.  For example:
$$
\etrans{}{}{@map f xs@} = app( app( @map@_{ptr}, @f@_{ptr}), @xs@)
$$
assuming that @map@ and @f@ are top-level functions in the $\theLang$-program, and
@xs@ is a local variable.  Once enough $app$ applications stack up, so that
$@map@_{ptr}$ is applied to two arguments, we can invoke the @map@ function directly 
in the logic, an idea we express with the following axiom:
$$
\forall x y.\;app(app(@map@_{ptr}, x), y) = @map@(x,y)
$$
These axioms, one for each function $f$, are generated by the second
clause of the rules for $\dtrans{}{d}$ in Figure~\ref{fig:etrans}.
(The notation $app(f,\ol{x})$ is defined in Figure~\ref{fig:fol-image}.)
You can think of $@map@_{ptr}$ as a ``pointer to'', or ``name of'' of, @map@.
The $app$ axiom for @map@ translates a saturated use of @map@'s pointer into
a call of @map@ itself.

%% DV: This paragraph confused 2 reviewers, and confuses me so I remove it.

%% This translation of higher-order functions to first-order logic may be
%% easy, but it is not complete. In particular, in first-order logic we
%% can only reason about functions with a concrete first-order
%% representation (i.e. the functions that we already have and their
%% partial applications) but, for example, lambda expressions cannot be
%% created during proof time. Luckily, the class of properties we reason
%% about (contracts) never require us to do so.

\subsection{Data types and {\tt case} expressions} \label{s:case-fol}

The second equation for $\utrans{}{u}{s}$ in Figure~\ref{fig:etrans} deals with
@case@ expressions, by generating a conjunction of formulae, as follows:
\begin{itemize*}
\item If the scrutinee $t$ is $bad$ (meaning that evaluating it invokes @BAD@) then
the result $s$ of the @case@ expression is also $bad$.  That is, @case@ is strict in
its scrutinee.
\item If the scrutinee is an application of one of the constructors $K_i$ mentioned
in one of the @case@ alternatives, then the result $s$ is equal to the corresponding
right-hand side, $e'_i$, after quantifying the variables $\ol{y}$ bound by the @case@ alternative.
\item Otherwise the result is $unr$.
The bit before the implication $\Rightarrow$ is just the
negation of the previous preconditions; the formula
  $t{\neq}K_1(\oln{{\sel{K}{1}}(t)}{})$
is the clumsy FOL way to say ``$t$ is not built with constructor $K_1$''.
\end{itemize*}
Why do we need the last clause? Consider the function @not@:
\begin{code}
  not :: Bool -> Bool
  not True = False
  not False = True
\end{code}
Suppose we claim that $@not@ \in @CF@ \rightarrow @CF@$, which is patently true.
But if we lack the last clause above, the claim 
is \emph{not} true in every model;
for example @not@ might crash when given the (ill-typed but crash-free) 
argument @3@.  The third clause above excludes this possibility by
asserting that the result of @not@ is the special crash-free constant $unr$
if the scrutinee is ill-typed (i.e. not $bad$ and not built with the 
constructors of the type).  This is the whole reason we need $unr$ in the first place.
In general, if $\etrans{}{}{e} = unr$ is provable in the logic, then $e$ is ill-typed, 
or divergent.

Of course, we also need to axiomatise the behaviour of data constructors and
selectors, which is done in Figure~\ref{fig:data-cons}:
\begin{itemize*}
\item \textsc{AxDisjCBU} explains that a term headed by a data constructor cannot
also be $bad$ or $unr$.
\item \textsc{AxInj} explains how the selectors $\sel{K}{i}$ work.
\item \textsc{AxDisjC} tells the prover that all data constructors are pairwise disjoint.
There are a quadratic number of such axioms, which presents a scaling problem.
For this reason FOL provers sometimes offer a built-in notion of data constructors,
so this is not a problem in practice, but we ignore this pragmatic issue here.
\end{itemize*}
A small point is that \rulename{AxCfBU} actually implies \rulename{AxDisjBU} but for 
presentational reasons we keep both axioms.

\kc{Something needs to be said about recursion. Higher-order functions are not the problem, but recursive definitions are. What we get in FOL are all fixpoints, but our denotational semantics only wants least fixpoints. Luckily, many things we want to prove hold for all fixpoints, but sometimes we need to invoke induction, which is not explicitly part of the translation. Translating all this into a higher-order logic (such as Isabelle) would allow us to express that we mean the least fixpoint.}

\subsection{Translation of contracts to FOL} \label{s:contracts-fol}

Now that we know how to translate \emph{programs} to first order
logic, we turn our attention to translating \emph{contracts}.  We do
not translate a contract \emph{per se}; rather we translate the claim
$e \in \Ct$.  Once we have translated $e \in \Ct$ to a first-order logic
formula, we can ask a prover to prove it using axioms from the translation of
the program, or axioms from Figure~\ref{fig:data-cons}. If successful, we can
claim that indeed $e$ does satisfy $C$.  Of course that needs proof,
which we address in Section~\ref{s:soundness}.

Figure~\ref{fig:contracts-minless} presents the translation
$\ctrans{}{\Gamma}{e \in \Ct}$; there are four equations corresponding
to the syntax of contracts in Figure~\ref{fig:syntax}.
The last three cases are delightfully simple and direct.  Conjunction of contracts
turns into conjunction in the logic; a dependent function contract turns
into universal quantification and implication; and the claim that $e$ is
crash-free turns into a use of the special term $\lcf{t}$ in the logic.
We discuss crash-freedom in Section~\ref{s:cf-fol}.

The first equation, for predicate contracts $e \in \{x \mid e' \}$,
is sightly more complicated.
The first clause $t=unr$, together with the axioms for $unr$ in Figure~\ref{fig:prelude}, ensures
that $unr$ satisfies every contract.
The second and third say that the contract holds if $e'$ diverges or is semantically
equal to @True@.  The choices embodied in this rule were discussed at length
in earlier work \cite{xu+:contracts} and we do not repeat it here.

\subsection{Crash-freedom} \label{s:cf-fol}

The claim $e \in @CF@$, pronounced ``$e$ is crash-free'', means that $e$ cannot
crash \emph{regardless of context}.  So, for example @(BAD, True)@ is not crash-free
because it can crash if evaluated in the context @fst (BAD, True)@.  Of course,
the context itself should not be the source of the crash; for example @(True,False)@ is
crash-free even though @BAD (True,False)@ will crash.

We use the FOL term $\lcf{t}$ to assert that $t$ is crash-free. The axioms for $\lcfZ$
are given in Figure~\ref{fig:prelude}.  \textsc{AxCfC} says that a data constructor application
is crash-free if and only iff its arguments are crash-free.  \textsc{AxCfBU} says that
$unr$ is crash-free, and that $bad$ is not.  That turns out to be all that we need.

\subsection{Summary}

That completes our formally-described --- but so far only informally-justified --- translation
from a $\theLang$ program and a set of contract claims, into first-order logic.
To a first approximation, we can now hand the generated axioms from the program and
our axiomatisation to an FOL theorem prover and ask it to use them to prove the 
translation of the contract claims. 


