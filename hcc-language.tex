To formalize the ideas behind our implementation, we define a
tiny source language $\cal L$:
polymorphic, higher-order, call-by-name $\lambda$-calculus with
algebraic datatypes, pattern matching, and recursion.  
Our actual implementation treats all of Haskell, by using GHC as a front 
end to translate Haskell into language $\cal L$.

\subsection{Syntax of $\cal L$} \label{s:syntax}

\begin{figure}
\[\begin{array}{l} 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Programs, definitions, and expressions}} \\
P   & ::= & d_1 \ldots d_n \\
d   & ::= & f\; \ol{a} \; \ol{(x\!:\!\tau)} = u \\
u   & ::= & e \; \mid \; @case@\;e\;@of@\;\ol{K\;\ol{y} -> e}
\end{array}
\\ 
\begin{array}{lrll}
e  & ::=  & x            & \text{Variables} \\ 
   & \mid & f[\ol{\tau}] & \text{Function variables} \\ 
   & \mid & K[\ol{\tau}](\ol{e}) & \text{Data constructors (fully applied)} \\
   & \mid & e\;e         & \text{Applications} \\
   & \mid & @BAD@        & \text{Runtime error} \\ 
\end{array}\\ \\ 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Syntax of closed values}} \\
 v,w & ::= & K^\ar[\ol{\tau}](\oln{e}{\ar}) \;\mid\; f^\ar[\ol{\tau}]\;\oln{e}{m < \ar} \;\mid\; @BAD@ \\ \\ 
\end{array}
\\ 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Contracts}} \\
 \Ct & ::=  & \{ x \mid e \}        & \text{Base contracts}  \\ 
     & \mid &  (x : \Ct_1) -> \Ct_2      & \text{Arrow contracts} \\ 
     & \mid & \Ct_1 \& \Ct_2             & \text{Conjunctions}   \\ 
     & \mid & \CF                        & \text{Crash-freedom}   \\
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Types}} \\
\tau,\sigma & ::=  & T\;\taus & \text{Datatypes} \\ 
            & \mid & a \mid \tau -> \tau 
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Type environments and signatures}} \\
\Gamma & ::=  & \cdot \mid \Gamma,x \\
\Delta & ::=  & \cdot \mid \Delta,a \mid \Delta,x{:}\tau \\
\Sigma & ::=  & \cdot \mid \Sigma,T{:}n \mid \Sigma,f{:}\forall\ol{a} @.@ \tau \mid \Sigma,K^{\ar}{:}\forall\ol{a} @.@ \oln{\tau}{\ar} -> @T@\;\as
\end{array}
\\ \\
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Auxiliary functions}} \\
%% constrs(\Sigma,T) & = & \{ K \mid (K{:}\forall \as @.@ \taus -> T\;\as) \in \Sigma \} \\
(\cdot)^{-}            & = & \cdot \\
(\Delta,a)^{-}         & = & \Delta^{-} \\
(\Delta,(x{:}\tau)^{-} & = & \Delta^{-},x
%% \tyar{D}{f} & = & n & \\ 
%%             & \multicolumn{3}{l}{\text{when}\; (f |-> \Lambda\oln{a}{n} @.@ \lambda\ol{x{:}\tau} @.@ u) \in D} \\
%% \tmar{D}{f} & = & n & \\ 
%%             & \multicolumn{3}{l}{\text{when}\; (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{n} @.@ u) \in D}
\end{array}
\end{array}\] 
\caption{Syntax of $\cal L$ and its contracts}\label{fig:syntax}
\end{figure}

Figure~\ref{fig:syntax} presents the syntax of $\cal L$.  A program
$P$ consists of a set of recursive function definitions $d_1 \ldots
d_n$. Each definition has a left hand side that binds its type-variable and
term-variable parameters;
if $f$ has $n$ term-variable parameters we say that 
it has arity $n$, and sometimes write it $f^n$.
The right hand side $u$ of a definition is either a @case@ expression or a
@case@-free expression $e$.  A @case@-free expression consists of
variables $x$, function variables $f[\taus]$ fully applied to their
type arguments, applications $e_1\;e_2$, data constructor applications
$K[\taus](\ol{e})$, as well as the special value @BAD@, which will be
used to model failure as a throwable error term. As a notation, we use
$\oln{x}{n}$ for sequences of elements of size $n$. When $n$ is
ommitted $\ol{x}$ has a size which is implied by the context or is not
interesting.

Our language embodies several convenient syntactic constraints: (i)~$\lambda$
abstractions occur only at the top-level, (ii)~@case@-expressions can
only immediately follow a function definition, and (iii) constructors
are fully applied.  These constraints do not restrict expressiveness;
lambda-lifting, @case@-lifting, and eta-expansion respectively can
easily transform a program with nested constructs and
partially-applied constructors into our restricted form.  Indeed our
prototype relies on existing implementation of similar transformations
from the GHC-as-a-library API. However this simpler language is
instead extremely convenient for the translation of programs to
first-order logic.

Figure~\ref{fig:syntax} embodies one other inessential choice in order to 
facilitate the formalization and implementation: we assume that 
functions have arity at least one (disallowing CAF's).  \spj{Disallowing CAFs is quite significant.
We need to say more about why.} \dv{It is not essential -- it was in some previous version of a proof. I will fix this.}

$\cal L$ is an explicitly-typed language, and we assume the 
existence of a typing relation $\Sigma |- P$, which checks 
that a program conforms to the definitions in the signature $\Sigma$. A signature $\Sigma$ (Figure~\ref{fig:syntax})
records the declared data types, data constructors and types of functions in the program $P$. The 
well-formedness of expressions is checked with a typing relation $\Sigma;\Delta |- u : \tau$, where $\Delta$
is a typing environment, also in Figure~\ref{fig:syntax}.
We do not give the details of the typing relation since it is standard. An additional property that we require 
from the typing relation is that it {\em asserts the exhaustiveness of pattern matches}. In an {\em actual}
source language programmers may ommit pattern matches but here we will assume that all pattern matches 
are exhaustive. Originally-incomplete cases have been completed to return the crashing term @BAD@. For 
instance, the program:
\begin{code}
head :: [a] -> [a]
head (x:xs) = x
\end{code}
will be represented in our language as:
\[\begin{array}{l}
   @head@\; a\; (x{:}[a]) = @case@\;x\;@of@ \{ [] -> @BAD@ ; (x:xs) -> x \} 
\end{array}\]
Finally, our technical development and analysis in the following sections assume that programs have been 
checked for type errors. 

The syntax of closed values is also given in Figure~\ref{fig:syntax}. Since we do not 
have arbitrary $\lambda$-abstractions, values can only be partial function applications
$f^\ar[\ol{\tau}]\;\oln{e}{m < \ar}$, data constructor applications $K[\tau](\ol{e})$, 
and the error term @BAD@. 

\subsection{Operational semantics of $\cal L$}

\spj{Why do we give an operational as well as denotational semantics?}
The big-step operational semantics of our language is given in 
Figure~\ref{fig:opsem}, which contains no surprises. One interesting
detail of big-step semantics is that they do not distinguish between non-termination 
and ``getting stuck'', meaning that if $P \not|- e \Downarrow v$ then $e$ could either diverge or its 
evaluation could get stuck. We return to this convenient for our purposes form of operational 
semantics later. \spj{Where later? Also this sentence is hard to parse; indeed I'm not quite
sure what it means.}
\begin{figure}
\[\begin{array}{c} 
\ruleform{P |- u \Downarrow v} \\ 
\prooftree
\begin{array}{c} \ \\ 
\end{array}
-------------------------------------{EVal}
P |- v \Downarrow v
~~~~~
\begin{array}{c}
(f \;\ol{a}\;\oln{(x{:}\tau)}{m} = u) \in P \\
P |- u[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \Downarrow v
\end{array}
-------------------------------------{EFun}
P |- f[\ol{\tau}]\;\oln{e}{m} \Downarrow v
~~~~~
\begin{array}{c}  
P |- e_1 \Downarrow v_1 \\
P |- v_1\;e_2 \Downarrow w
\end{array}
------------------------------------------------{EApp}
P |- e_1\;e_2 \Downarrow w
~~~~~
\begin{array}{c}  
P |- e_1 \Downarrow @BAD@ 
\end{array}
------------------------------------------------{EBadApp}
P |- e_1\;e_2 \Downarrow @BAD@
~~~~~
%% \endprooftree \\ \\ 
%% \ruleform{P |- u \Downarrow v} \\ \\
%% \prooftree
%% P |- e \Downarrow v
%% -------------------------------------{EUTm}
%% P |- e \Downarrow v
%% ~~~~ 
\begin{array}{c}
P |- e \Downarrow K_i[\ol{\sigma}_i](\ol{e}_i) \quad
P |- e'_i[\ol{e}_i/\ol{y}_i] \Downarrow w
\end{array}
------------------------------------{ECase}
P |- @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'} \Downarrow w
~~~~~
\begin{array}{c}
P |- e \Downarrow @BAD@ \\
\end{array}
------------------------------------{EBadCase}
P |- @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'} \Downarrow @BAD@
%% \begin{array}{c}
%% (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{m} @.@ @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}) \in D \\
%% D |- e[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \Downarrow @BAD@ \\
%% \end{array}
%% -------------------------------------{EBadCase}
%% D |- f[\ol{\tau}]\;\oln{e}{m} \Downarrow @BAD@
\endprooftree
\end{array}\]
\caption{Operational semantics of $\cal L$}\label{fig:opsem}
\end{figure}
%% We can state some standard properties of the typing and evaluation relation.
%% \begin{lemma}[Subject reduction]
%% Assume $\Sigma |- P$ and $\Sigma;\cdot |- e : \tau$
%% If $P |- e \Downarrow w$ then $P |- value(w)$ and $\Sigma;\cdot |- w : \tau$.
%% \end{lemma}
The operational semantics of Figure~\ref{fig:opsem} has the possibility of non-deterministism because
of the overlapping of several rules for applications. But that is not a problem, as we can prove that evaluation 
is deterministic using the following two lemmas.
\begin{lemma}[Value determinacy]
If $\Sigma;\cdot |- v : \tau$ and 
$\Sigma |- P$ and $P |- v \Downarrow w$ then $ v = w $.
\end{lemma}
\begin{lemma}[Determinacy of evaluation]
If $\Sigma;\cdot |- e : \tau$ and 
$\Sigma |- P$ and $\Sigma;\cdot |- e \Downarrow v_1$ and $\Sigma;\cdot |- e \Downarrow v_2$ then
$v_1 = v_2$.
\end{lemma}
Finally, big-step soundness asserts that an expression that evaluates results in a
well-typed value.
\begin{lemma}[Big-step soundness]
If $\Sigma;\cdot |- e : \tau$ and 
$\Sigma |- P$ and $\Sigma;\cdot |- e \Downarrow v$ then $\Sigma;\cdot |- v : \tau$.
\end{lemma}

%% Figure~\ref{fig:syntax} also presents the syntax of {\em contracts} to
%% keep all essential definitions grouped together, but in this section
%% we will only focus on the language and its semantics. In
%% Section~\ref{sect:contracts} we return to the syntax and semantics of
%% contracts.

%% \subsection{Denotational semantics}\label{ssect:denot}

%% Having presented the language $\cal L$ and its operational semantics, we now return to 
%% our roadmap which is to axiomatize and use the denotational semantics of programs to 
%% perform static contract checking. In what follows we will assume a program $P$, 
%% well-formed in a signature $\Sigma$, so that $\Sigma |- P$. Most of what follows is 
%% an adaptation of folklore techniques to our setting and there are no 
%% surprises -- we refer the reader to~\cite{winskel} or~\cite{benton+:coq-domains} 
%% for a short and modern exposition of the standard methodology.

%% Given a signature $\Sigma$ we define a strict bi-functor $F$ on complete partial 
%% orders (cpos), below:
%% %% For a well-formed signature $\Sigma$, we define the strict bi-functor on cpos, below, 
%% %% assuming that $K_1\ldots K_k$ are all the constructors in $\Sigma$: 
%% \[\begin{array}{lclll}
%%   F(D^{-},D^{+}) & = & ( \quad{\prod_{\ar_1}{D^{+}}} & K_1^{\ar_1} \in \Sigma \\
%%                & + & \;\quad\ldots                    & \ldots \\
%%                & + & \;\quad{\prod_{\ar_k}{D^{+}}} & K_k^{\ar_k} \in \Sigma \\ 
%%                & + & \;\quad(D^{-} =>_c D^{+}) \\
%%                & + & \;\quad\unitcpo_{bad} \quad )_{\bot}
%% \end{array}\]
%% \spj{Why bi-functor? Why not just functor?}%% \dv{I am using Pitts' and Nick Benton's formulation and this 
%% %% thing needs to be a bifunctor. It is not a functor if you have contra-varianance.} 
%% The bi-functor $F$ is the lifting of a big sum: that sum consists of 
%% (i) products, one for each possible constructor (even across different data types), (ii) the continuous
%% function space from $D^{-}$ to $D^{+}$, and (iii) a unit cpo to denote @BAD@ values. 
%% The notation $\prod_{n}{D}$ abbreviates $n$-ary products of cpos (the unit cpo $\unitcpo$ if $n = 0$). 
%% The product and sum constructions are standard, but note that we use their non-strict versions. 
%% The notation $C =>_c D$ denotes the cpo 
%% induced by the space of continuous functions from the cpo $C$ to the cpo $D$. We use 
%% the notation $\unitcpo_{bad}$ to 
%% denote a single-element cpo -- the $bad$ subscript is just there for readability. 
%% The notation $D_\bot$ is {\em lifting}, which is a monad, equipped with the following two continuous 
%% functions.
%% \[\begin{array}{l}
%%    \ret   : D =>_c D_\bot \\ 
%%    \bind_{f : D =>_c E_\bot} : D_\bot =>_c E_\bot
%% \end{array}\]
%% with the obvious definitions. 

%% Observe that we have dropped all type information from the source language. The elements of the products 
%% corresponding to data constructors are simply $D^{+}$ (instead of more a precise description from type 
%% information) and the return types of data constructors are similarly ignored. This is not to say that 
%% a more type-rich denotational semantics is not possible (or desirable even) but this simple denotational 
%% semantics turns out to be sufficient for formalization and verification. 

%% %% for $\lambda$-abstractions and @BAD@. Observe that we have 
%% %% Moreover, the following continuous operations are defined:
%% %% \[\begin{array}{l} 
%% %%    \curry_{f : D\times E =>_c F} : D =>_c (E =>_C F) \\ 
%% %%    \eval : (E =>_c D)\times E =>_c D 
%% %% \end{array}\] 
%% %% for any cpos $D, E, F$.

%% \spj{What is ``the equation induced by F''?  Maybe $$D_{\infty} = F( D_{\infty}, D_{\infty})$$? 
%% Let's write it down.}
%% Using the standard {\em embedding-projection} pairs methodology we can show the following.
%% \begin{lemma}\label{lem:rec-solution} 
%% There exists a solution to the domain-recursive equation induced by $F$, call it $D_{\infty}$.
%% Moreover, let a value-domain: $V_{\infty}$ be defined as:
%%     \[V_{\infty} = \begin{array}[t]{ll}
%%              \quad\;{\prod_{\ar_1}{D_{\infty}}} & K_1^{\ar_1} \in \Sigma \\
%%              \; + \;\ldots                    & \ldots \\
%%              \; + \;{\prod_{\ar_k}{D_{\infty}}} & K_k^{\ar_k} \in \Sigma \\ 
%%              \; + \;(D_{\infty} =>_c D_{\infty}) \\
%%              \; + \;\unitcpo_{bad} \quad
%%     \end{array}\]
%% The following continuous functions also exist, each being the inverse of the 
%% other (i.e. composing to the identity function on the corresponding cpo):
%% \[\begin{array}{l}
%%   \roll : (V_{\infty})_\bot =>_c D_{\infty} \\ 
%%   \unroll : D_{\infty} =>_c (V_{\infty})_\bot
%% \end{array}\] 
%% \end{lemma}


%% \paragraph{Definability of application}
%% We may now {\em define} application $\dapp : D_\infty \times D_\infty =>_c D_\infty$ as a continuous
%% function: 
%% {\setlength{\arraycolsep}{2pt}
%% \[\begin{array}{rcll}
%%    \dapp & = & \multicolumn{2}{l}{\dlambda d @.@ \roll(\bind_g (\unroll (\pi_1(d))))} \\
%%    \text{ where } g & = &  [ & \bot : \prod_{\ar_1}{D_\infty} =>_c D_\infty =>_c (V_\infty)_\bot \\
%%                     &   &  , & \ldots \\
%%                     &   &  , & \bot : \prod_{\ar_k}{D_\infty} =>_c D_\infty =>_c (V_\infty)_\bot \\
%%                     &   &  , & \dlambda d' @.@ \unroll(d'(\pi_2(d))) \\
%%                     &   &  , & \dlambda b @.@ \dlambda d. \ret(\inj{bad}(b))\hspace{2pt} ] 
%% \end{array}\]}%
%% Informally, what does the $\dapp$ combinator express? If the first
%% argument is $\bot$ it will return $\bot$, if the first argument is an
%% injection $\inj{bad}$ it will return the same, if it corresponds to a
%% data constructor it will return $\bot$. Finally, if the first element
%% is indeed a function, it will apply it.
%% \spj{Why not use $\dapp(fun, arg) = ...$ rather than all this stuff with angle brackets and pis. 
%% That just adds clutter.}
%% As a notational convention, we have used notation $\langle , \rangle$ to introduce pairs and $[\ldots]$ to 
%% eliminate ($n$-ary) sums. 
%% \spj{I don't understand the $[\ldots]$ notation.}
%% The projections $\pi_1$ and $\pi_2$ are the obvious continuous projections from the 
%% binary product space of $D_{\infty}$. We use notation $\inj{K}$ to denote the continuous map that injects some $n$-ary product of $D_{\infty}$ 
%% corresponding to the arity of constructor $K$ into the sum $V_{\infty}$.
%% We use notation $\inj{->}$ to denote the continuous injection of $(D_{\infty} =>_c D_{\infty})$ into $V_{\infty}$ and finally, 
%% $\inj{bad}$ for the unit injection into $V_{\infty}$. We use ordinary application notation 
%% $d(d')$ \spj{instead of $app(d,d')$?} \dv{No, instead of the @apply@ combinator of the cpo $D_\infty =>_c D_\infty$, which is different
%% than $\dapp$ which has a completely different signature than @apply@ (its arguments are both elements of $D_\infty$)} when $d : D_\infty =>_c D_\infty$ and
%% $d'$ is an element of $D_\infty$. \spj{Why inconsistent notation? Why not say $d':D_{\infty}$?}
%% Indeed application, partial application, and currying are all definable in cpos of continuous functions, 
%% so we will be using $\lambda$-calculus notation for our domain theory, as above. 


%% \paragraph{Denotational semantics of expressions and programs}
%% =======
\subsection{Contracts}


\begin{figure}
\[\begin{array}{lcl}
e \in \{ x\;\mid\;e_p\} & <=> &  e \not\Downarrow \text{ or } e_p[e/x] \not\Downarrow \text{ or} \\ 
                        &     &  e_p[e/x] \Downarrow True \\
e \in (x{:}\Ct_1) -> \Ct_2 & <=> & 
                        \forall e' \in \Ct_1.\; (e\;e') \in \Ct_2[e'/x] \\
e \in \Ct_1 \& \Ct_2 & <=> & e \in \Ct_1 \text{ and } e \in \Ct_2 \\
e \in \CF            & <=> & \forall {\cal C}. BAD \not\in {\cal C} \Rightarrow e \not\Downarrow BAD
\end{array} 
\]
\caption{Operational semantics of contracts} \label{f:contract-spec-op}
\end{figure}

We now turn our attention to contracts. The syntax of contracts
is given in Figure~\ref{fig:syntax} and includes base contracts
$\{ x \mid e \}$, arrow contructs $(x : \Ct_1) -> \Ct_2$, conjunctions
$\Ct_1 \& \Ct_2$ and crash-freedom $\CF$. Previous work~\cite{xu+:contracts} 
includes other constructs as well, but these constructs are enough to verify 
many programs and already demonstrate the interesting theoretical and practical problems.

We write $e \in C$ to mean ``the expression $e$ satisfies the contract $C$'', and similarly
for functions $f$.
Figure~\ref{f:contract-spec-op} says what it means to say $e \in C$,
based closely on earlier work \cite{xu}.  The specification is
simple, declarative, and intended to be comprehensible to programmers.
For example, an expression $e$ satisfies the function contract $\Ct_1 \& \Ct_2$ if and 
only if $(e e')$ satisfies $\Ct_2$ whenever $e'$ satisfies $Ct_1$.

Crucially, base contracts $\{x|e\}$ allow arbitrary $\cal L$
expressions (in our implementation, arbitrary Haskell expressions),
rather than being restricted to some well behaved meta-language.  This
is great for the programmer because the language and its library
functions is familiar, but it poses a challenge for verification
because these expressions in contracts may themselves diverge or
crash.


% -----------------------------------------------------------------
\section{Translating $\cal L$ to first-order logic} \label{ssect:denot-fol}

Our goal is to answer the question ``does expression $e$ satisfy
contract $C$?''.  Our plan is to translate both the expression and the
contract into first-order logic (FOL), and get a standard FOL prover
to do the heavy lifting.  

In this section we formalise our new translation, and describe how we use it to
verify contracts.

\subsection{The FOL language}

\begin{figure}
\[\begin{array}{c} 
\begin{array}{lrll}
\multicolumn{3}{l}{\text{Terms}} \\
  s,t & ::=  & x                          & \text{Variables} \\ 
      & \mid & f(\ol{t})                  & \text{Function applications} \\
      & \mid & K(\ol{t})                  & \text{Constructor applications} \\ 
      & \mid & \sel{K}{i}(t)              & \text{Constructor selectors} \\ 
      & \mid & f_{ptr} \mid app(t,s)       & \text{Pointers and application} \\
      & \mid & \unr \mid \bad             & \text{Unreachable, bad} \\ \\
\multicolumn{3}{l}{\text{Formulae}} \\ 
 \phi & ::=  & \lcf{t}    & \text{Crash-freedom} \\
%%      & \mid & \lncf{t}   & \text{Can provably cause crash} \\
      & \mid & t_1 = t_2  & \text{Equality} \\ 
      & \mid & \phi \land \phi \mid \phi \lor \phi \mid \neg \phi \\
      & \mid & \forall x @.@ \phi \mid \exists x @.@ \phi \\ \\ 
\end{array}
\\
\multicolumn{1}{l}{\text{Abbreviations}} \\ 
\begin{array}{rcl}
app(t,\oln{s}{n} & = & (\ldots(app(t,s_1),\ldots s_n)\ldots) \\
\phi_1 \Rightarrow \phi_2 & = & \neg \phi_1 \lor \phi_2
\end{array}
\end{array}\]
\caption{Syntax of FOL}\label{fig:fol-image}
\end{figure}

We begin with the syntax of the FOL language, which is given in
Figure~\ref{fig:fol-image}. There are two syntactic forms,
\emph{terms} and \emph{formulae}. Terms include function applications
$f(\ol{t})$, constructor applications $K(\ol{t})$, variables. They
also include, for each data constructor $K^\ar$ in the signature
$\Sigma$ with arity $\ar$ a set of {\em selector functions}
$\sel{K}{i}(t)$ for $i \in 1 \ldots \ar$.  The terms $app(t,s)$ and
$f_{ptr}$ concern the higher-order aspects of $\cal L$, which we
discuss in (see Section~\ref{s:hof}).  Finally we introduce two new
syntactic constructs $\unr$ and $\bad$. As an abreviation we often use
$app(t,\ol{s})$ for the sequence of applications to each $s_i$, as
Figure~\ref{fig:fol-image} shows.

The formulae of Figure~\ref{fig:fol-image} is just first-order logic
with equality, plus a predicate $\lcf{t}$ for crash-freedom, whose
semantics we discuss in Section~\ref{sect:contracts}.

\subsection{Translation of expressions to FOL} \label{s:hof}

% ---------------------------------------------------
\begin{figure}\small
\setlength{\arraycolsep}{2pt} 
\[\begin{array}{c} 
\ruleform{\etrans{\Sigma}{\Gamma}{e} = \formula{t} } \\ \\
\begin{array}{rcl}
\etrans{\Sigma}{\Gamma}{x} & = & \formula{x} \\
\etrans{\Sigma}{\Gamma}{f[\ol{\tau}]} & = & \formula{f_{ptr}} \\
\etrans{\Sigma}{\Gamma}{K[\ol{\tau}](\ol{e})} & = & \formula{K(\ol{\etrans{e}{\Gamma}{t}})} \\
\etrans{\Sigma}{\Gamma}{e_1\;e_2} & = & \formula{app(\etrans{\Sigma}{\Gamma}{t_1},
                                                     \etrans{\Sigma}{\Gamma}{t_1})} \\
\etrans{\Sigma}{\Gamma}{@BAD@} & = & \formula{\bad}
\end{array}
\\ \\
\ruleform{\utrans{\Sigma}{u}{s} = \formula{\phi}} \\ \\
\begin{array}{rcl}
\utrans{\Sigma}{e}{s}
  & = & \formula{(s = \etrans{\Sigma}{\Gamma}{e})} \\
\multicolumn{3}{l}{\utrans{\Sigma}
    {@case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}}{s}} \\
\multicolumn{3}{l}{
\quad 
  \begin{array}[t]{rl}
    = & \formula{(t = \bad => s = bad)} \\ 
    \land & \formula{(\forall \ol{y} @.@ t = K_1(\ol{y}) => s = \etrans{\Sigma}{\Gamma}{e'_1})\;\land \ldots}  \\
    \land & \formula{(t{\neq}\bad\;\land\;
                 t{\neq}K_1(\oln{{\sel{K_1}{i}}(t)}{})\;\land\;\ldots) => s{=}\unr} \\
    \mbox{where} & t  =  \etrans{\Sigma}{\Gamma}{e} 
 \end{array}
}
\end{array}
\\ \\
\ruleform{\dtrans{\Sigma}{d} = \formula{\phi}} \\ \\
\begin{array}{rcl}
  \dtrans{\Sigma}{f \;\ol{a}\;\ol{(x{:}\tau)} = u} 
    & =     & \formula{\forall \ol{x} @.@ \utrans{\sigma}{u}{f(\ol{x})}} \\
    & \land & \formula{\forall \ol{x} @.@ f(\ol{x}) = app(f_{ptr},\xs)} \\
\end{array}
\\ \\
\ruleform{\ptrans{\Sigma}{P} = \formula{\phi} } \quad
\ptrans{\Sigma}{\ol{d}} = \bigwedge \ol{\dtrans{\Sigma}{d}}
\\ \\
\ruleform{\ctrans{\Sigma}{\Gamma}{e \in \Ct} = \formula{\phi}} \\ \\ 
\begin{array}{rcl}
\ctrans{\Sigma}{\Gamma}{e \in \{(x{:}\tau) \mid e' \}} 
  & = & \formula{t{=}\unr} \\ 
  & \lor & \formula{t'[t/x]{=}\unr} \\
  & \lor & \formula{t'[t/x]{=}\True} \\
  & \mbox{where} & 
    \begin{array}[t]{rcl}
      t  & = & \etrans{\Sigma}{\Gamma}{e} \\
      t' & = & \etrans{\Sigma}{\Gamma}{e} 
    \end{array}
\\
\ctrans{\Sigma}{\Gamma}{e \in (x{:}\Ct_1) -> \Ct_2} 
  & = & \formula{\forall x @.@ \ctrans{\Sigma}{\Gamma,x}{x \in \Ct_1} 
                          \Rightarrow \ctrans{\Sigma}{\Gamma,x}{e\;x \in \Ct_2}}
\\
\ctrans{\Sigma}{\Gamma}{e \in \Ct_1 \& \Ct_2} 
   & = & \formula{ \ctrans{\Sigma}{\Gamma}{e \in \Ct_1} /\ \ctrans{\Sigma}{\Gamma}{e \in \Ct_2}}
\\
\ctrans{\Sigma}{\Gamma}{e \in \CF} & = & \formula{\lcf{\etrans{\Sigma}{\Gamma}{e}}}
\end{array}
\end{array}\]
\caption{Translation of programs and contracts to logic}\label{fig:etrans}
\end{figure}
% ---------------------------------------------------
\begin{figure}\small
\setlength{\arraycolsep}{1pt} 
\[\begin{array}{c} 
\begin{array}{ll} 
\multicolumn{2}{l}{\text{Axioms for fixed constants}} \\
 \textsc{AxAppBad}  & \formula{\forall x @.@ app(\bad,x){=}\bad}  \\
 \textsc{AxAppUnr}  & \formula{\forall x @.@ app(\unr,x){=}\unr}    \\
 \textsc{AxDisjBU} & \formula{\bad \neq \unr}  \\ 
 \textsc{AxCfBU} & \formula{\lcf{\unr} /\ \lncf{\bad}} \\
\\
\multicolumn{2}{l}{\mbox{Axioms for data constructors}} \\
 \textsc{AxCfC}  & \formula{\forall \oln{x}{n} @.@ \lcf{K(\ol{x})} <=> \bigwedge\lcf{\ol{x}}} \\
                 & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
 \textsc{AxDisjC} & \formula{\forall \oln{x}{n}\oln{y}{m} @.@ K(\ol{x}) \neq J(\ol{y})} \\ 
                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ 
                  & \text{ and } (J{:}\forall\as @.@ \oln{\tau}{m} -> S\;\as) \in \Sigma \\
 \textsc{AxDisjCBU} & \formula{(\forall \oln{x}{n} @.@ K(\ol{x}) \neq \unr \land K(\ol{x}) \neq \bad)} \\ 
                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ \\
 %% \textsc{AxAppB}  & \formula{\forall \oln{x}{n} @.@ K(\ol{x}) = app(\ldots (app(x_K,x_1),\ldots,x_n)\ldots)} \\
 %%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
 %% Not needed: we can always extend partial constructor applications to fully saturated and use AxAppC and AxDisjC
 %% \textsc{AxPartA} & \formula{\forall \oln{x}{n} @.@ app(\ldots (app(x_K,x_1),\ldots,x_n)\ldots) \neq \unr} \\
 %%                  & \formula{\quad\quad \land\; app(\ldots (app(x_K,x_1),\ldots,x_n)\ldots) \neq \bad} \\
 %%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{m} -> T\;\as) \in \Sigma \text{ and } m > n \\
 \textsc{AxInj}   & \formula{\forall \oln{y}{n} @.@ \sel{K}{i}(K(\ys)) = y_i} \\ 
                  & \text{for every } K^\ar \in \Sigma \text{ and } i \in 1..n
\end{array}
\end{array}\]
\caption{Axioms of the FOL constants}\label{fig:prelude}
\end{figure}

% ---------------------------------------------------
\spj{Is this next paragraph right?  I want to set the scene.}
What exactly does it mean to translate an expression to first-order logic?
We are primarily interested in reasoning about equality, so we might
hope for this (informally):
$$
e_1 =_S e_2 \;<=>\; \etrans{}{}{e_1} =_{FOL} \etrans{}{}{e_2}
$$
where $=_S$ means semantic equality, $=_{FOL}$ means equality in first-order logic,
and $\etrans{}{}{e}$ is the translation of $e$ to a FOL term. That is, we can 
reason about the equality of Haskell terms by translating them into FOL using
a FOL theorem prover. 

The translation of programs, definitions, and expressions to FOL
is given in Figure~\ref{fig:etrans}.
The function $\ptrans{}{P}$ translates a program to a conjunction of formulae,
one for each definition $d$, while $\dtrans{}{d}$ in turn translates 
a definition $d$.
The first forumula in $\dtrans{}{}$'s right-hand side invokes the translation
$\utrans{}{u}{f(\ol{x})}$ for the right hand side $u$, passing in the term $f(\ol{x})$,
and quantifying over the $\ol{x}$.  We will deal with the second formula shortly.

Ignoring @case@ for now (which we discuss in Section~\ref{s:case-fol}), $\utrans{}{u}{s}$
simply returns a formula asserting the equality $s = \etrans{}{}{e}$. 
That is, we use a new constant $f$ in the logic for each function definition in the
program, and assert that any application of $f$ is equal to (the logical translation of)
$f$'s right hand side. Notice that we erase type arguments in the translation
since they do not affect the semantics.

Lastly $\etrans{}{}{e}$ deals with expressions.  If $\cal L$ was 
a first-order language the translation would be easy:
$$
\etrans{\Sigma}{\Gamma}{f[\ol{\tau}]\;\ol{e}} = \formula{f(\ol{\etrans{}{}{e}})} \\
$$
At first it might be surprising that we can also translate a \emph{higher-order} language
$\cal L$ into first order logic, but in fact it is easy to do so, as 
Figure~\ref{fig:etrans} shows.  We introduce into the logic
(a) a single new constant $app$, standing
for application, and (b) a nullary constant $f_{ptr}$ for each function $f$ 
(see Figure~\ref{fig:fol-image}).
Then, the equations for $\etrans{}{}{}$ translate application in $\cal L$ to 
a use of $app$ in FOL, and any mention of function $f$ in $\cal L$ to a use
of $f_{ptr}$ in the logic.  For example:
$$
\etrans{}{}{@map f xs@} = app( app( @map@_{ptr}, @f@_{ptr}), @xs@)
$$
assuming that @map@ and @f@ are top-level functions in the $\cal L$-program, and
@xs@ is a local variable.  Once enough $app$ applications stack up, so that 
$@map@_{ptr}$ is applied to two arguments, can invoke @map@ directly in the logic,
an idea we expression with the following axiom:
$$
\forall x y.\;app(app(@map@_{ptr}, x), y) = @map@(x,y)
$$
These axioms, one for each function $f$, are generated by the second
clause of the rules for $\dtrans{}{}{}$ in Figure~\ref{fig:etrans}.
(The notation $app(f,\ol{x})$ is defined in Figure~\ref{fig:fol-image}.)
You can think of $@map@_{ptr}$ as a ``pointer to'', or ``name of'' of, @map@.
The $app$ axiom for @map@ translates a saturated use of @map@'s pointer into
a call of @map@ itself.   

\subsection{Data types and {\tt case} expressions} \label{s:case-fol}

The second equation for $\utrans{}{u}{}$ in Figure~\ref{fig:etrans} deals with
@case@ expressions, by generating a conjuntion of formulae, as follows:
\begin{itemize}
\item If the scrutinee $t$ is $bad$ (meaning divergence or a use of @error@) then
the result $s$ of the @case@ expression is also $bad$.  That is, @case@ is strict in
its scrutinee.
\item If the scrutinee is an application of one of the constructors $K_i$ mentioned
in one of the @case@ alternatives, then the result $s$ is equal to the corresponding
right-hand side (after quantifying the variables bound by the @case@).
\item Otherwise the result is $unr$.  The formula 
  $t{\neq}K_1(\oln{{\sel{K_1}{i}}(t)}{})$ 
is just the clumsy FOL way to say ``$t$ is not built with constructor $K_1$.
\end{itemize}
The most interesting rule is \rulename{DCase}. If the translation of the scrutinee
$t$ is equal to $\bad$ then the left-hand-side $s$ must be equal to $\bad$. If it is 
equal to some constructor application from the case expressions then the right 
hand-side is equal to the corresponding translation $t_K$ of some pattern branch. Finally
if the scrutinee is not $\bad$ nor some of the constructors in the branch we assert that
the left-hand-side $s$ must be equal to $\unr$. The rule precisely corresponds to our 
intuitions from the denotational semantics: in the denotational semantics, whenever
we attempt to scrutinize a constructor of a different type in a case expression, the 
resulting denotation is simply $\bot$. This reinforces our desire to treat type errors
as $\bot$ values. 

\subsection{Translation of contracts to FOL}

Figure~\ref{fig:prelude} gives an axiomatization. For notational convenience the reader
should think that each of these rules can potentially give a conjunction of formulae.
The axiom \rulename{AxDisjBU} asserts that $\bad$ cannot be equated to $\unr$. 
Rule \rulename{AxDisjC} is really givin a conjunction of axioms. It asserts that all 
constructors are disjoint with each other and \rulename{AxDisjCBU} asserts that they 
are also disjoint from $\bad$ and $\unr$. 
Axiom group \rulename{AxPtr} asserts that full applications of function pointers 
are equal to the actual function applications and \rulename{AxApp} describes the behavior
of $app(\cdot,\cdot)$ when the first argument is $\bad$ or $\unr$. Finally the selector
functions describe how we can project a value out of a datatype. 


We now turn to expressing contract semantics in first-order logic. 
We do not translate a contract; rather we translate the claim $e \in C$.
Once we have translated $e \in \Ct$ to a first-order logic formula, we can
ask the prover to prove it; and, if successful, we can claim that indeed
$e$ does satisfy $C$.  Of course that needs proof, which we address in Section~\ref{xxx}.

Figure~\ref{fig:contracts-minless} presents the details of the 
function $\ctrans{}{\Gamma}{e \in \Ct}$.  We also include 
an axiomatization of crash-freedom with the axiom groups \rulename{AxCfBU} and 
\rulename{AxCfC}. The first axiom asserts that $\unr$ is crash-free and $\bad$ is not, 
whereas the second axiom asserts that a constructor value is crash-free if and only iff 
all of its children nodes are crash-free. 

\subsection{Bad, unreachable, and crash-freedom}

\spj{desribe these}

\subsection{Proving that a function satisfies a contract}

\spj{How to actually prove that a function satisfies a contract.  Or maybe
it's enough to forward-reference the implementation section}
