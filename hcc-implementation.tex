Our prototype contract checker is called \textbf{Halo}.
It uses GHC to parse, typecheck, and desugar a Haskell program,
translates it into first order logic (exactly as in Section~\ref{ssect:trans-fol}), and
invokes a FOL theorem prover (Equinox, Z3, Vampire, etc) on the FOL formula.
The desugared Haskell program is expressed in GHC's intermediate language
called Core~\cite{Sulzmann:2007:SFT:1190315.1190324}, an explicitly-typed 
variant of System F.  It is straightforward
to translate this into our language $\theLang$.

\subsection{Using the tool}

How does the user express contracts? We use HOAS and let the user
define contracts with a library of contract combinators, reminiscent 
of the work on {\em typed contracts} for functional 
programming~\cite{Hinze:2006:TCF:2100071.2100093}.
The contract combinators are defined in a generalized algebraic 
datatype @Contract t@, which represents a contract for a function of type @t@:
\begin{code}
data Contract t where
  (:->) :: Contract a 
        -> (a -> Contract b)
        -> Contract (a -> b)
  Pred  :: (a -> Bool) -> Contract a
  CF    :: Contract a
  (:&:) :: Contract a -> Contract a -> Contract a
\end{code}
The combinators are: @:->@ for dependent contract function space, @CF@
for crash-freedom, @Pred@ for predication, and
@:&:@ for conjunction. A useful derived contract is for the 
non-dependent function space:
\par {\small
\begin{code}
(-->) :: Contract a -> Contract b -> Contract (a -> b)
c1 --> c2 = c1 :-> \_ -> c2
\end{code}
} \par
%As one would expect, @:->@ and @-->@ are right-associve.  We can
%create contract combinators that are always satisfied, and never
%satisfied:
%
%\begin{code}
%any :: Contract a
%any = Pred (\ _ -> True)
%
%never :: Contract a
%never = Pred (error "never!")
%\end{code}

A contract is always associated with a function, so we wrap a contract
together with its function in a @Statement@:
\begin{code}
data Statement where
    (:::) :: a -> Contract a -> Statement
\end{code}
For example, in our previous notation we might write the following
contract for @head@:
$$
@head@ \in \CF \;\&\; \{@xs@ \mid @not (null xs)@ \} \rightarrow \CF
$$
Here is how we express the contract as a Haskell definition:
\begin{comment}
head (x:xs) = x
head []     = error "empty list"

not True = False    null [] = True
not False = True    null xs = False

f . g = \x -> f (g x)
\end{comment}
\begin{code}
c_head :: Statement
c_head = head ::: CF :&: Pred (not . null) --> CF
\end{code}
Upon saving this file to, say, @Head.hs@ and running @halo Head@,
@halo@ translates the contract (and the supporting definitions) into
FOL, generates a TPTP file and invokes a theorem prover.
In fact @c_head@ is verified by all theorem provers we tried.

We apply fixpoint induction on contracts for recursive functions, as
described in Section~\ref{s:induction}.

% ------------------------ Omit ----------------------------
\begin{comment}
\subsection{Recursion}

We prove the contract for a recursive function
using fixed point induction (Section~\ref{s:induction}).
For recursive
functions, the tool then gives three TPTP files, one that can be used to try to 
prove the contract without induction, one for the base case and step case. The
typical situation is that the one without induction is not provable, because it 
lacks the appropriate induction hypothesis. The base case always succeeds 
(because $\bot$ satisfies every contract as we have seen in Section~\ref{s:induction})
so it really onle serves as a sanity check of the tool. 
The induction step case may pass or fail, depending on if the
contract really holds, and if the induction hypothesis is strong
enough, and if we have assumed the right contracts for functions that may be used
in the body of the function that we are working on.

One example of a recursive function in the Prelude is @foldr1@.

\begin{code}
foldr1          :: (a -> a -> a) -> [a] -> a
foldr1 f [x]    =  x
foldr1 f (x:xs) =  f x (foldr1 f xs)
foldr1 _ []     =  error "foldr1: empty list"
\end{code}

We can state that if @foldr1@ is applied to a crash free function, and
a non-empty list, then the result should be crash free as a contract:
\begin{code}
c_foldr = foldr1 ::: (CF --> CF --> CF) 
                 --> CF :&: Pred (not . null) --> CF
\end{code}
Our tool proves this contract, but only when recursion is used.
\end{comment}
% ------------------------ End Omit ----------------------------

\subsection{Practical considerations}

To make the theorem prover work as fast as possible we trim the
theories to include only what is needed to prove a
property. Unnecessary function pointers, data types and definitions
for the current goal are not generated.

When proving a series of contracts, it is natural to do so in dependency order.
For example:
\begin{code}
  reverse :: [a] -> [a]
  reverse [] = []
  reverse (x:xs) = reverse xs ++ [x]

  reverse_cf :: Statement
  reverse_cf = reverse ::: CF --> CF
\end{code}
To prove this contract we must first prove that
$@(++)@ \in \CF \rightarrow \CF \rightarrow \CF$;
then we can prove @reverse@'s contract assuming the one for @(++)@.
At the moment we ask the programmer to specify which auxiliary contracts
are useful, via a second constructor in the @Statement@ type:
\begin{code}
  reverse_cf = reverse ::: CF --> CF
                       `Using` append_cf
\end{code}


\subsection{Dependent contracts}

Here is an example of a contract about @filter@,
where the result depends on an earlier argument, the predicate @p@:
$$
@filter@ \in (@p@ : \CF \rightarrow \CF) \rightarrow
             \CF \rightarrow \CF \;\&\; \{@ys@ \mid @all p ys@ \}
$$
This contract says that under good crash-free circumstances, the
all elements in the resulting list from filtering a list with @p@
satisfies @p@.
In our source-file syntax we use @(:->)@ to bind @p@.
\begin{code}
filter_all :: Statement
filter_all = 
  filter ::: (CF --> CF) :-> \p ->
                CF --> (CF :&: Pred (all p))
\end{code}
It is a little confusing since we get two ``arrows'', one from @:->@,
and one from the @->@ in the lambda.  This contract is proved by
applying fixed point induction.

\subsection{Higher order functions}

Our tool also deals with higher order functions containing
very higher-order functions. Consider this (real) function @withMany@ from the
@GHC@ component @Foreign.Util.Marshal@:
%\footnote{\url{http://hackage.haskell.org/packages/archive/base/latest/doc/html/Foreign-Marshal-Utils.html#v:withMany}}:

\begin{code}
withMany :: (a -> (b -> res) -> res)
         -> [a] -> ([b] -> res) -> res
withMany _       []     f = f []
withMany withFoo (x:xs) f = withFoo x (\x' ->
      withMany withFoo xs (\xs' -> f (x':xs')))
\end{code}

For @withMany@, our tool proves

\[ @withMany@ \in \begin{array}[t]{l} (\CF -> (\CF -> \CF) -> \CF) -> \\
                                      \quad\quad (\CF -> (\CF -> \CF) -> \CF)
                  \end{array}\] 

% ------------------------ Omit ----------------------------
\begin{comment}
\subsection{A small case-study about invariants}

We consider a somewhat non-standard way of expressing propositional
logic formulae:

\begin{code}
data Formula = And [Formula]
             | Or  [Formula]
             | Neg (Formula)
             | Implies (Formula) (Formula)
             | Lit Bool
\end{code}

One invariant that we are particularily interested in is that we
should never have two consecutive negations, and that the lists of
@And@ and @Or@ are of length $\ge$ 2. We can express that as an ordinary
Haskell predicate:

\begin{code}
invariant :: Formula -> Bool
invariant f = case f of
  And xs      -> properList xs && all invariant xs
  Or xs       -> properList xs && all invariant xs
  Neg Neg{}   -> False
  Neg x       -> invariant x
  Implies x y -> invariant x && invariant y
  Lit x       -> True

properList :: [a] -> Bool
properList []  = False
properList [_] = False
properList _   = True
\end{code}

Now, we have a recursive function that negates formula:

\begin{code}
neg :: Formula -> Formula
neg (Neg f)         = f
neg (And fs)        = Or (map neg fs)
neg (Or fs)         = And (map neg fs)
neg (Implies f1 f2) = neg f2 `Implies` neg f1
neg (Lit b)         = Lit b
\end{code}

We make a combinator saying what it means to retain a predicate:

\begin{code}
retain :: (a -> Bool) -> Contract (a -> a)
retain p = Pred p :-> \x -> Pred (\r -> p x && p r)
\end{code}

\dr{TODO: explain this. This was DV's brilliant idea but I still don't
  fully understand it} Now, since @neg@ uses @map@, we need to show that
@map@ can retain the invariant. We use @all@, introduced above, for
this:

\begin{code}
map_invariant = map ::: retain invariant -->
                        retain (all invariant)
\end{code}

Explicitly spelling out the definition of @retain@ in the statement
above would be tedious and error-prone, so we see the benefit of being
able to express contracts as a DSL.

We can now express that @neg@ retains the invariant:

\begin{code}
neg_contr = neg ::: retain invariant
  `Using` map_invariant
\end{code}

We use @Using :: Statement -> Statement -> Statement@, another
constructor for @Statement@, which allows us to assume that other
contracts holds, when proving a complicated statement, thus
our Statement data type really looks like this:

\begin{code}
data Statement where
    (:::) :: a -> Contract a -> Statement
    Using :: Statement -> Statement -> Statement
\end{code}

For now, it's the user's responsibility to prove these assumed
contracts (for instance, with our tool!), but one can imagine a more
sophisticated front-end which does this automatically.  Note that
the assumption in @neg_contr@ is necessary. If we remove it, and
use the min-translation, we a finitely counter satisfiable theory.
\end{comment}
% ------------------------ End of omit ----------------------------


% \subsection{Example: shrink}
%
% Recall that @fromJust@ is the partial function @Maybe a -> a@, and consider
% this code:
%
% \begin{code}
% shrink :: (a -> a -> a) -> [Maybe a] -> a
% shrink op []     = error "Empty list!"
% shrink op [x]    = fromJust x
% shrink op (x:xs) = fromJust x `op` shrink op xs
% \end{code}
%
% Is this contract satisfied for it?
% \begin{code}
%     (CF --> CF --> CF) -->
%     (CF :&: Pred nonEmpty :&: Pred (all isJust)) --> CF
% \end{code}

\subsection{Experimental Results}

\input{hcc-resultstable}

We have made some initial experimental results, a small fragment of which can be 
viewed in Figure~\ref{fig:unsres}. Our full testuite and tables can be downloaded from
\url{https://github.com/danr/contracts/blob/master/tests/BigTestResults.md}.
\dv{Is this right? How many examples did we evaluate on? 10? 20? 300? is it imporant?}
``Trivial'' contracts used for sanity checking of the tool are not listed. 
Here are some parts of what is covered:
\begin{itemize}
  \item Crash freeness of standard functions
    (@(++)@, @foldr1@, @iterate@, @concatMap@).

  \item CF of functions with a bit more involving recursive patterns
        (Ackermann's function, functions with accumulators),

%  \item A library for predicate logic terms with some smart constructors
%        retaining an invariant,
%
%        % \dr{Everything failed from this without min so I just skip it}

  \item Partial functions given appropriate preconditions
        (@foldr1@, @head@, @fromJust@),

  \item The @risers@ example from Catch~\citep{Mitchell:2008:PBE:1411286.1411293},

  \item Some properties, such as the example above with @filter@ and @all@,
        and also $@any p xs || any p ys@ = @any p (xs ++ ys)@$.
\end{itemize}

We tried four theorem provers, Equinox, Z3, Vampire and E, and gave
them 60 seconds for each problem. For our problems, Z3 seems to be the most
successful theorem prover.

