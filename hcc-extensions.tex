So far we have described the basic translation of the denotational semantics, programs, and contracts
to first-order logic. However, to enable verification of contracts in practice we must consider two 
important extensions, outlined in the rest of this section.

\subsection{A heuristic for minimization of countermodels}\label{sect:minimization}

For a query of the form $\Th \land \Th_\lcfZ \land \neg \ctrans{}{P}{e \in \Ct}$, a SAT solver will search for
a model. When such a model exists, it will nclude tables for the function symbols in the formula. Notice that functions 
in FOL are total over the domain of the terms in the model. This means that function tables may be {\em infinite} if the 
terms in the model are infinite. Several (very useful!) axioms such as the discrimination axioms \textsc{AxDisjC} may in 
fact force the models to be infinite. For instance consider the following devinitions:
\begin{code}
length [] = Z
length (x:xs) = S (length xs)

isZero Z = True
isZero _ = False
\end{code}
Suppose that we would like to check that 
   \[ @length@ \in \CF -> \{ x \mid @isZero@\;x\} \]
which is a falsifiable contract.  A satisfiability-based checker 
will simply diverge trying to construct a counter model for the negation of the above query; we 
have confirmed that this is indeed the behaviour of several tools (Z3, Equinox, Eprover).
Indeed the table for @length@ is infinite since @[]@ is always disjoint from @Cons x x@ for 
any @x@ and @xs@. Even if there is a finitely representable infinite model there is always the 
possibility of the theorem prover searching in the ``wrong corner'' of the model for a 
counterexample with no success. 

From a practical point of view this is {\em not acceptable}: After all, there exists a very simple 
counterexample that demonstrates the problem, e.g. @[Z]@, and we only need the 
functions of our program to be defined on a {\em finite} number of values (those that appear 
during the evaluation of this problematic counterexample) to be able to demonstrate 
the problem. We simply {\em do not care} about values that a function may take outside 
the set of expressions that appear during the finite evaluation of a counterexample.

This is a challenge that we solve by modifying our axiomatization of the semantics 
and the translation of programs and contracts in a way that it can still admit 
the $\langle D_\infty,{\cal I}\rangle$ model, but also some {\em finite} model in 
the case that a counterexample exists. 

To achieve this effect, we introduce a predicate $min(\cdot)$ that, intuitively, is true
for the terms that have been evaluated during the execution of a counterexample. We use the
name $min$ because the purpose of this predicate is to minimize countermodels. We often 
refer to the terms in the $min(\cdot)$ predicate as the ``set of terms we are interested in''.

\begin{figure} 
{\small
\[\setlength{\arraycolsep}{1pt}
\begin{array}{c}
\ruleform{\Th_\infty^{min}} \\ \\ 
\begin{array}{lll}
 \textsc{AxDisjBU} & \formula{\bad \neq \unr} \\ 
 \textsc{AxDisjC} & \formula{\forall \oln{x}{n}\oln{y}{m} @.@} \\ 
                  & \formula{\;\;\highlight{min(K(\ol{x}))\;\lor\;min(J(\ol{y}))} =>
                                  K(\ol{x}){\neq}J(\ol{y})} \\
                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ 
                  & \text{ and } (J{:}\forall\as @.@ \oln{\tau}{m} -> S\;\as) \in \Sigma \\
 %% \textsc{AxDisjCUnr} & \formula{\forall \oln{x}{n} @.@ \highlight{\neg min(\unr)}} \\ 
 %%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ \\
 \textsc{AxDisjCU} & \formula{\forall \oln{x}{n} @.@ \highlight{min(K(\ol{x}))} => K(\ol{x}) \neq unr} \\
 \textsc{AxDisjCB} & \formula{\forall \oln{x}{n} @.@ K(\ol{x}) \neq \bad} \\ 
 %% \textsc{AxDisjCBad} & \formula{\forall \oln{x}{n} @.@ K(\ol{x}) \neq \bad} \\ 
 %%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ \\

 \textsc{AxPtr}  & \formula{\forall \oln{x}{n} @.@ \highlight{min(app(f_{ptr},\xs))} => f(\ol{x}) = app(f_{ptr},\xs)} \\
                 & \text{ for every } (f |-> \Lambda\as @.@ \lambda\oln{x{:}\tau}{n} @.@ u) \in P \\
 %% \textsc{AxAppB}  & \formula{\forall \oln{x}{n} @.@ K(\ol{x}) = app(\ldots (app(x_K,x_1),\ldots,x_n)\ldots)} \\
 %%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
 \textsc{AxApp}  & \formula{\forall x, app(\bad,x) = \bad \; /\ \; app(\unr,x) = \unr}    \\ 
 \textsc{AxAppMin}& \formula{\highlight{\forall x, min(app(x,y)) => min(x)}} \\ 
 %% Not needed: we can always extend partial constructor applications to fully saturated and use AxAppC and AxDisjC
 %% \textsc{AxPartA} & \formula{\forall \oln{x}{n} @.@ app(\ldots (app(x_K,x_1),\ldots,x_n)\ldots) \neq \unr} \\
 %%                  & \formula{\quad\quad \land\; app(\ldots (app(x_K,x_1),\ldots,x_n)\ldots) \neq \bad} \\
 %%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{m} -> T\;\as) \in \Sigma \text{ and } m > n \\
 %% \textsc{AxPartB} & \formula{\forall \oln{x}{n} @.@ app(f_{ptr},\xs) \neq \unr} \\
 %%                  & \formula{\quad\land\; app(f_{ptr},\xs) \neq \bad} \\
 %%                  & \formula{\quad\land\; \forall \oln{y}{k} @.@ app(f_{ptr},\xs) \neq K(\ol{y})} \\
 %%                  & \text{ for every } (f |-> \Lambda\as @.@ \lambda\oln{x{:}\tau}{m} @.@ u) \in P  \\
 %%                  & \text{ and every } (K{:}\forall\as @.@ \oln{\tau}{k} -> T\;\as) \in \Sigma \text{ and } m > n  \\ \\ 
 \textsc{AxInj}   & \formula{\forall \oln{y}{n} @.@ \highlight{min(\sel{K}{i}(K(\ys)))}} \\ %% \;\land\; min(y_i)}} \\ 
                  & \formula{\quad\qquad\qquad => \sel{K}{i}(K(\ys)) = y_i} \\ 
                  & \text{for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \text{ and } i \in 1..n 
\end{array}
%% \ruleform{\Th_\lcfZ^{min}} \\ \\ 
%% \begin{array}{lll} 
%%  \textsc{AxCfBU}  & \formula{\lcf{\unr} /\ \lncf{\bad}} \\
%%  \textsc{AxCfMin} & \formula{\highlight{\forall x @.@ \lcf{x} => min(x) \lor x = unr}} \\
%%  %% \textsc{AxCfB1}   & \formula{\forall \oln{x}{n} @.@ \bigwedge_i (\lcf{x_i}\lor \neg(min(x_i))} => \lcf{K(\ol{x})} \lor \neg(min(K(\ol{x}))) \\
%%  %%                   & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ 
%%  \textsc{AxCfC1} & \formula{\forall \oln{x}{n} @.@ \bigwedge\lcf{\ol{x}}} => \lcf{K(\ol{x})} \\
%%                  & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
%%  \textsc{AxCfC2} & \formula{\highlight{min(K(\oln{x}{n}))\land\neg\lcf{K(\oln{x}{n})}}} \\ 
%%                  & \formula{\quad\qquad\qquad \highlight{ => \bigvee_i (min(x_i)\land\neg\lcf{x_i})}}
%% \end{array}
\end{array}\]}
\caption{A theory with minimization}\label{fig:min-theory}
\end{figure}

Figure~\ref{fig:min-theory} presents a variation of $\Th_\infty^{min}$ which includes minimization.
In this figure, we have highlighted the parts where our theory differs compared to $\Th_\infty$ from
Figure~\ref{fig:prelude}. The first intersting axiom group is \rulename{AxDisjC} where instead of 
unconditionally asserting that two constructors are disjoint, we assert that they are disjoint if
one of the two values is a member of the $min(\cdot)$ predicate. Intuitively, evaluation of a 
counterexample has touched one of the two constructors (for instance, by performing a pattern matching
on a value of this constructor) and hence we should consider this constructor value different than any 
other one. Similarly \rulename{AxDisjCBU} asserts that a constructor value is not $\unr$ nor $\bad$ if 
it is an interesting value. This leaves the possibility of a model where all elements that are not in 
the $min(\cdot)$ set have been conflated to a single value. In \rulename{AxPtr}, if evaluation of a counterexample
has touched an appliation of $app(f_{ptr},\ol{x})$ then we should be able to derive the equality 
$f(\ol{x}) = app(f_{ptr},\ol{x})$, which will further allow us to gain knowledge about the definition of the function.
The intuition behind \rulename{AxAppMin} is easy: if the term $min(app(x,y))$ has appeared in the evaluation trace, 
we have definitely evaluated the argument and hence we have $min(x)$. Finally, the selector axiom group \rulename{AxInj}
has been modified to guard the selector value as one would expect.

\begin{figure}\small
\[\begin{array}{c}
\ruleform{\uutrans{\Sigma}{\Gamma}{t \sim u}^{min} = \formula{\phi}} \\ \\ 
\prooftree
   \begin{array}{c}
   \etrans{\Sigma}{\Gamma}{e} = \formula{t}
   \end{array}
   ----------------------------------------{DExp}
   \begin{array}{l} 
   \uutrans{\Sigma}{\Gamma}{s \sim e }^{min} = \formula{\highlight{min(s)} => (s = t)} 
   \end{array}
   ~~~~~
  \begin{array}{l}
  \etrans{\Sigma}{\Gamma}{e} = \formula{t} \\
  %% constrs(\Sigma,T) = \ol{K} \\
  \text{for each branch}\;(K\;\oln{y}{l} -> e') \text{ it is } 
  %% \begin{array}{l}
  %%          (K{:}\forall \cs @.@ \oln{\sigma}{l} -> T\;\oln{c}{k}) \in \Sigma \text{ and }
           \etrans{\Sigma}{\Gamma,\ol{y}}{e'} = \formula{ t_K }
  %% \end{array}
  \end{array}
  ------------------------------------------{DCase}
  {\setlength{\arraycolsep}{1pt} 
  \begin{array}{l}
  \uutrans{\Sigma}{\Gamma}{s \sim @case@\;e\;@of@\;\ol{K\;\ol{y} -> e'}}^{min} = \\
  \;\;\formula{ \begin{array}{l} 
     \highlight{min(s)} => \\
     \begin{array}{ll}
          ( & \highlight{min(t)}\;\land \\
            & (t = \bad => s = \bad)\;\land \\ 
            & (\forall \ol{y} @.@ t = K_1(\ol{y}) => s = t_{K_1})\;\land \ldots \land \\
            & (t{\neq}\bad\;\land\;t{\neq}K_1(\oln{{\sel{K_1}{i}}(t)}{})\;\land\;\ldots => s{=}\unr) \\
          )
%% (t = \bad /\ s = \bad)\;\lor\;(s = \unr)\;\lor \\
%%                                 \quad      \bigvee(t = K(\oln{{\sel{K}{i}}(t)}{}) \land
%%                                            s = t_K[\oln{\sel{K}{i}(t)}{}/\ol{y}])
                   \end{array}
     \end{array}}
  \end{array}}
  %% {       \setlength{\arraycolsep}{2pt} 
  %% \begin{array}{l}
  %% \utrans{\Sigma}{\Gamma}{s \sim @case@\;e\;@of@\;\ol{K\;\ol{y}{->}e'}} = \\
  %% \;\;\formula{
  %%      \begin{array}{l} (\highlight{s{=}\unr})\;\lor \\ 
  %%                           \;\; (\highlight{min(s) => min(t)}\;\land  \\
  %%                           \quad((t = \bad /\ s = \bad)\;\lor \\
  %%                           \quad\quad \bigvee(t = K(\oln{{\sel{K}{i}}(t)}{}) \land
  %%                                          s = t_K[\oln{\sel{K}{i}(t)}{}/\ol{y}])))
  %%                  \end{array}
  %%          }
  %% \end{array}}
\endprooftree
\end{array}\]
\caption{Program translation with minimization}\label{fig:min-def-trans-min}
\end{figure}
\newcommand{\ThMin}{\Th_{\infty}^{min}}
\newcommand{\SDownarrow}{\downarrow}

The translation of programs to accomodate minimization requires only modification 
to the $\uutrans{}{\Gamma}{u}$ judgement, which now become $\uutrans{}{\Gamma}{u}^{min}$. Its definition
is given in Figure~\ref{fig:min-def-trans-min}. Rule \rulename{DExp} is unfolding a function
defininion only if the result of the function is in the $min(\cdot)$ set. Rule \rulename{DCase}
has the same flavor. However if we have $min(s)$ then the focus of evaluation in the counterexample
will move on to the scrutinee of the case expression, and hence we get a $min(t)$ predicate, where the 
term $t$ is the FOL translation of the case scrutinee $e$.

An easy property of this translation is that it yields a theory that still admits the denotational model 
when we interpret $min$ as the everywhere-true predicate:
\begin{theorem} $\langle D_\infty, {\cal I}\uplus min |-> \lambda d.true\rangle \models \ThMin$. \end{theorem} 

However, we shall later prove that, unlike $\Th_\infty$, the new theory $\ThMin$ does admit in many practical 
cases finite models along with infinite ones (such as $\langle D_{\infty},{\cal I}\rangle$), and these finite 
models are directly induced from finite traces of failed programs.



\subsection{Minimization in contracts}
\newcommand{\ctransmin}[3]{\ctrans{#1}{#2}{#3}^{min}} 
\newcommand{\calI}{{\cal J}}

We have showed that, given a strict trace, our new theory admits a finite model. But what is its proving 
power? In order to prove a contract we may have to appeal to some axiom from the theory. But, alas, most
axioms in $\ThMin$ are guarded by $min(\cdot)$ predicates, effectively casting them unusable if we are 
trying to prove contracts that arise from the translation in Figure~\ref{fig:contracts-minless}.

Our solution to this problem is to modify the translation of contracts as well to use $min(\cdot)$ predicates.
The modified contract translation as well as an axiomatization of $\Th_\lcfZ^{min}$ is in Figure~\ref{fig:min-typing}.
An important deviation compared to our previous contract translation in Figure~\ref{fig:contracts-minless} is the
splitting of the contract translation to a positive variant $\ctransmin{}{\Gamma}{e \in \Ct}$ and a negative 
variant $\ctransmin{}{\Gamma}{e \notin \Ct}$. The reason for this is a subtle interaction of the $min(\cdot)$ 
predicates. 

Our goal will be to find a contradiction to $\ctransmin{}{\Gamma}{e \notin \Ct}$, so we start by explaining
the negative variant. Observe that the formula $\ctransmin{}{\Gamma}{e \notin \Ct}$ 
always give rise to a conjunction of $min$ predicates as the base cases (rule \rulename{NCBase} and \rulename{NCCf})
assert $min$ predicates. Consider this very simple example:
\begin{code}
g x  = True 
\end{code} 
and we wish to show that $g$ satisfies $\CF -> \CF$, we may assert the existence of a crash-free $x$, such that 
$app(g,x)$ is not crash-free. However, recall that the axioms that we have generated for $g$ are: 
\[\begin{array}{l} 
   \forall x. min(app(g_{ptr},x)) => app(g_{ptr},x) = g(x) \\ 
   \forall x. min(g(x)) => g(x) = True
\end{array}\] 
This means that, if we have no information that $min(app(g_{ptr},x))$ we are not in a position 
to assert that the result is $True$ which is crash-free! In fact for any value $y$ that is not 
in the $min$ predicate there is a countermodel, e.g. $g(y) = \bad$. Therefore the formula 
$\ctransmin{}{\Gamma}{e \notin \Ct}$ inserts enough $min$ predicates in order to ``drive'' 
the application of function axioms from the assumptions. In our example this manifests as: 
\[\begin{array}{l} 
   \ctransmin{}{}{g \notin \CF -> \CF} = \\
  \exists x @.@ \ctransmin{}{}{x \in \CF} \land min(app(g_{ptr},x)) \land \\ 
  \quad\;\; \neg\lcf{app(g_{ptr},x)}
\end{array}\] 
which is now unsatisfiable, as required. 

The positive variant $\ctransmin{}{\Gamma}{e \in \Ct}$ is first introduced when 
trying to derive a contradiction for an arrow contract (rule \rulename{NCTrans}): 
\[\begin{array}{l}
   \ctransmin{\Sigma}{\Gamma}{e \notin (x{:}\Ct_1) -> \Ct_2}  = \\
    \exists x @.@ \highlight{\ctransmin{\Sigma}{\Gamma,x}{x \in \Ct_1}} \land \ctransmin{\Sigma}{\Gamma,x}{e\;x \notin \Ct_2}
\end{array}\]
It corresponds to adding an assumption $\ctransmin{\Sigma}{\Gamma,x}{x \in \Ct_1}$ and trying to 
derive a contradiction for the conclusion of the arrow contract. 

Designing an effective positive variant for practical verification turns out to be the subject of 
delicate design choices. Let us consider the base case \rulename{CBase}. One naive approach would 
be to simply assign: 
\[\begin{array}{l}
     \ctransmin{}{\Gamma}{e \in \{ x \mid e' } = \\ 
     \quad (t{=}\unr) \lor (t'[t/x]{=}\unr) \lor (t'[t/x]{=}\True)
\end{array}\]
where $t'$ is the FOL translation of $e'$ and $t$ is the FOL translation of $e$. 
However, in many practical examples we may have to use the truth value of a predicate to actually learn something 
about its argument. Consider the following example:
\begin{code}
f True  = True
f False = error "bad!"
\end{code}
and suppose we would like to prove that @f@ satisfies the contract $\CF \& \{ y \mid (id\;y) \} -> \CF$. Intuitively, 
the precondition should only be satisfied by the $\True$ value or $\unr$. However let us examine what happens. We try
to derive a contradiction from:
\[\begin{array}{l}
    \exists x @.@ \ctransmin{}{\Gamma}{x \in \CF} \land \ctransmin{}{\Gamma}{x \in \{ y \mid (id\;y)\}} \land \\
    \phantom{\exists x \;\;} \ctransmin{}{\Gamma}{f\;x \notin \CF} 
\end{array}\] 
With the aforementioned definition of positive contracts, 
a theorem prover will try to come up with some crash free $x$ such that $app(id,x) = \unr \lor app(id,x) = True$
and such that $f\;x$ is not crash-free. However, observe that the value $False$ will satisfy the above requirements! 
We do not have available a predicate $min(app(id,x))$ and hence we cannot reduce $app(id,x)$ to $x$ and learn that $x$ 
can only be $\unr$ or $True$. Hence the contract has become satisfiable whereas this simple function really does belong
in the denotational semantics of its contract! 

The solution to this problem is to state that we are actually {\em interested} in the value of the predicate. Formally, 
we add a $min(t'[t/x])$ predicate that will now allow us to use information from the value of the predicate $t'[t/x]$ 
to learn information about $t$:
\[\begin{array}{l}    \ctransmin{}{\Gamma}{e \in \{ x \mid e' } = \\ 
   \quad \highlight{min(t'[t/x]} \land \\ 
   \quad \quad ((t{=}\unr) \lor (t'[t/x]{=}\unr) \lor (t'[t/x]{=}\True))
\end{array}\]
Sadly, this choice has an unfortunate consequence: there are now {\em too many} terms that belong in 
the $min$-predicate. Consider for instance that the following positive contract has been produced:
\[\begin{array}{l}
    \ctransmin{}{\Gamma}{e \in \Ct_1 -> \{x \mid e'\}} = \\ 
        \quad \forall x. \ctransmin{}{\Gamma}{x \notin \Ct_1} \lor (min(t'[t/x]) \land \ldots )
\end{array}\] 
where again $t$ is the translation of $e$ and $t'$ is the translation of $e'$. This axiom is problematic
from an efficiency point of view as it asserts that {\em for every} x, either it does not belong in $\Ct_1$
or we have $min(t'[t/x])$. This means that way too many values may start appearing in the $min(\cdot)$ tables,
which in turn can cause uncontrollable instantiation of axioms and generation of new terms. 
Indeed a few higher-order examples that require induction demonstrate an unacceptable
degradation in performance using the above variation of the rule. 

Observe however that we should never really have to learn the value of a predicate $t'[t/x]$ unless we 
are actually interested in the value $t$ above. If not, it is quite unlikely that any information about
$t'[t/x]$ will result in some progress in finding a contradiction. Guided by this intuition, we further modify 
the rule \rulename{CBase} to introduce a {\em guard} which allows the rule to trigger only if $t$ itself is
in the $min$ predicate: 
\[\begin{array}{l}
    \ctransmin{}{\Gamma}{e \in \{ x \mid e' } = \\ 
    \highlight{min(t) => } \\ 
    \quad (\highlight{min(t'[t/x])} \land \\ 
    \quad\quad \land ((t{=}\unr) \lor (t'[t/x]{=}\unr) \lor (t'[t/x]{=}\True)))
\end{array} \]
and this is our final variant of rule \rulename{CBase}.

Similar design choices apply for $\CF$ contracts: in the negative case we assert that we are interested
in a term $t$ (rule \rulename{NCCf}) and try to derive a contradiction to the negation of crash-freedom.
In the positive case we guard crash-freedom by a $min$ assumption. Some delicate choices however have still
been made in the axiomatization of crash-freedom in Figure~\ref{fig:min-typing}. In particular we have now
two substantially different axioms than we had in the $min$-less world:
\[\begin{array}{lll} 
 \textsc{AxCfMin} & \formula{\highlight{\forall x @.@ \lcf{x} => min(x) \lor x = unr}} \\
 %% \textsc{AxCfB1}   & \formula{\forall \oln{x}{n} @.@ \bigwedge_i (\lcf{x_i}\lor \neg(min(x_i))} => \lcf{K(\ol{x})} \lor \neg(min(K(\ol{x}))) \\
 %%                   & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ 
 \textsc{AxCfC2} & \formula{\highlight{min(K(\oln{x}{n}))\land\neg\lcf{K(\oln{x}{n})}}} \\ 
                 & \formula{\quad\qquad\qquad \highlight{ => \bigvee_i (min(x_i)\land\neg\lcf{x_i})}}
\end{array}\]
\dv{We need one example here as well.} 
Axiom \rulename{AxCfMin} asserts that if we have managed to prove that a value is crash-free 
then {\em at some point} we have been interested in that value, or that it is $\unr$. Axiom 
\rulename{AxCfC2} asserts that if we are interested in a constructor value $K(\ol{x})$ which is 
nevertheless not crash-free, we know that there must exist some $x_i$ which we have been intrested 
in, and is nevertheless crash-free.

\paragraph{Soundness}
These choices are extremely delicate and have been derived very carefully from our intuitions about 
evaluation traces as well as practical experience from examples, but there is actually a very large 
design space in the placement of $min$. Fortunately for many variations of these rules, and in particular
for the set that we have chosen to present in this paper, soundness with respect to the denotational 
semantics is not difficult to see. 

The following lemma connects the positive and negative translations to 
the original translation $\ctrans{}{\Gamma}{e \in \Ct}$. 

\begin{lemma}\label{lem:contract-min} Assume a model $\langle M,\calI\rangle$ such that $\calI(min)(d)$ holds for every $d \in M$. 
Then the following are true, assuming that that $dom(\Gamma) \subseteq dom(\calI)$:
\begin{itemize*} 
  \item If $\langle M,\calI\rangle \models \neg \ctransmin{}{\Gamma}{e \notin \Ct}$ then $\langle M,\calI\rangle\models \ctrans{}{\Gamma}{e \in \Ct}$
  \item If $\langle M,\calI\rangle \models \ctrans{}{\Gamma}{e \in \Ct}$ then $\langle M,\calI\rangle \models \ctransmin{}{\Gamma}{e \in \Ct}$.
\end{itemize*}
\end{lemma}
%% \begin{proof} We prove the two cases simultaneously by induction on the structure of the contract $\Ct$:
%% \begin{itemize*}
%%   \item For the first part, the cases for rules \rulename{NCBase}, \rulename{NNCf}, \rulename{CConj} are straightforward. The only 
%%         interesting case is the \rulename{NCArr}, where we have that 
%%         \[\begin{array}{l}
%%              \neg \ctransmin{}{\Gamma}{e \in (x : \Ct_1) -> \Ct_2}  = \\ 
%%              \quad\quad \forall x @.@ \neg \ctransmin{}{\Gamma,x}{x \in \Ct_1} \lor \neg \ctransmin{}{\Gamma,x}{e\;x \notin \Ct_2}
%%         \end{array}\]
%%         Pick an $d$ and assume $\ctrans{}{\Gamma,x}{x \in \Ct_1}$ holds in the model extened with $x |-> d$. 
%%         By induction hypothesis (second case) it must be the case that $\ctransmin{}{\Gamma,x}{x \in \Ct_1}$ in this model, and hence $\neg \ctransmin{}{\Gamma,x}{e\;x \notin \Ct_2}$.
%%         By induction hypothesis then (first case) we get $\ctrans{}{\Gamma,x}{e\;x \in \Ct_2}$ as required.
%%   \item The second part is symmetric by appealing in the \rulename{CArr} case to the induction hypotheses for both sides.
%% \end{itemize*} 
%% \end{proof}

The following is an easy observation. 
\begin{lemma}\label{lem:min-model} If $\langle M,\calI\rangle$ is a model of $\Th \land \Th_\lcfZ \land \dtrans{}{P}$ then the same model, extended with
$\calI_m(min)(d) = true$ for every $d \in M$, is a model of $\ThMin \land \Th_\lcfZ^{min} \land \dtrans{}{P}^{min}$. 
\end{lemma}
However the next theorem is the most important result about minimization. 
\begin{theorem}[Soundness of min translation]\label{thm:min-soundness} If $\ThMin\land \Th_{\lcfZ}^{min} \land \dtrans{}{P}^{min} |- \neg \ctransmin{}{}{e \notin \Ct}$ then 
                   $\Th \land \Th_{\lcfZ} \land \dtrans{}{P} |- \ctrans{}{}{e \in \Ct}$ and hence $\dbrace{\Ct}(\dbrace{e})$.
\end{theorem}
\begin{proof}
Pick a model $\langle M, \calI\rangle$ of $\Th \land \Th_{\lcfZ} \land \dtrans{}{P}$ and extend its interpretation 
so that $\calI(min)(d) = true$. By Lemma~\ref{lem:min-model} this means that the the extended model is a model of 
$\ThMin\land \Th_{\lcfZ}^{min} \land \dtrans{}{P}^{min}$ and hence this is also a model of $\neg \ctransmin{}{}{e \notin \Ct}$. 
By Lemma~\ref{lem:contract-min} (first case) this is also a model of $\ctrans{}{}{e \in \Ct}$.
\end{proof} 
Theorem~\ref{thm:min-soundness} shows that we may simply generate $\ThMin \land \Th_{\lcfZ}^{min} \land \dtrans{}{P}^{min} \land \neg \ctransmin{}{}{e \notin \Ct}$
and ask a SAT-solver for a model of this formula. If it is unsatisfiable, then its negation is valid and from the theorem we learn that $\dbrace{\Ct}(\dbrace{e})$.


\paragraph{Completeness}

Is it true that everything that we can prove using 
the $min$-less theory, is also provable using the $min$-enabled translation? 
That is most likely not the case, although we are not aware of counterexamples. However, as we show
in the Section~\ref{hcc-evaluation} we have identified that our heuristics for minimization work much 
better than the $min$-less version: For the 90 small programs in our testsuite, the theorem provers 
finish on average faster when a contract is provable, and a finite model checker always finds a finite 
model in the case where there exists a counterexample in the $min$-enabled translation.


\paragraph{Finite model guarantees} 

\begin{figure}\small
\[\begin{array}{c} 
\ruleform{\ctransmin{\Sigma}{\Gamma}{e \in \Ct} = \formula{\phi}} \\ \\ 
\prooftree
  \begin{array}{c}
   \etrans{\Sigma}{\Gamma}{e} = \formula{t} \quad
   \etrans{\Sigma}{\Gamma,x}{e'} = \formula{t'}
  \end{array}
  ------------------------------------------{CBase}
  \begin{array}{l}
   \ctransmin{\Sigma}{\Gamma}{e \in \{(x{:}\tau) \mid e' \}} = \\
  %% \Sigma;\Gamma |- e \in \{(x{:}\tau \mid e' \}
   \quad \formula{\highlight{min(t) => (min(t'[t/x])\;}\land} \\ 
   \quad \formula{\quad ((t{=}\unr) \lor (t'[t/x]{=}\unr) \lor (t'[t/x]{=}\True)))}
  \end{array}
  ~~~~~ 
  \begin{array}{c}
  \ctransmin{\Sigma}{\Gamma,x}{x \notin \Ct_1} {=} \formula{\phi_1} \quad
  \ctransmin{\Sigma}{\Gamma,x}{e\;x \in \Ct_2} {=} \formula{\phi_2}
  \end{array} 
  ------------------------------------------{CArr}
  \begin{array}{l} 
  \ctransmin{\Sigma}{\Gamma}{e \in (x{:}\Ct_1) -> \Ct_2} = 
  \formula{\forall x @.@ \phi_1 \lor \phi_2}
  \end{array}
  ~~~~~
  \begin{array}{c}
  \ctransmin{\Sigma}{\Gamma}{e \in \Ct_1} = \formula{ \phi_1} \quad
  \ctransmin{\Sigma}{\Gamma}{e \in \Ct_2} = \formula{ \phi_2}
  \end{array}
  ------------------------------------------{CConj}
  \ctransmin{\Sigma}{\Gamma}{e \in \Ct_1 \& \Ct_2} = \formula{ \phi_1 /\ \phi_2}
  ~~~~~
  \etrans{\Sigma}{\Gamma}{e} =  \formula{t}
  -------------------------------------------{CCf}
  \ctransmin{\Sigma}{\Gamma}{e \in \CF} = \formula{\highlight{min(t)} => \lcf{t}}
 \endprooftree  \\ \\ 
\ruleform{\ctransmin{\Sigma}{\Gamma}{e \notin \Ct} = \formula{\phi}} \\ \\ 
\prooftree
  \begin{array}{c}
   \etrans{\Sigma}{\Gamma}{e} = \formula{t} \quad
   \etrans{\Sigma}{\Gamma,x}{e'} = \formula{t'}
  \end{array}
  ------------------------------------------{NCBase}
  \begin{array}{l}
   \ctransmin{\Sigma}{\Gamma}{e \notin \{(x{:}\tau) \mid e' \}} = \\
   \quad\formula{\highlight{min(t)\;\land\;min(t'[t/x])}\;\land } \\
   \quad\formula{\;\;((t{\neq}\unr) \land ((t'[t/x]{\neq}\unr) \land (t'[t/x]{\neq}\True)))}
  \end{array}
  ~~~~~ 
  \begin{array}{c}
  \ctransmin{\Sigma}{\Gamma,x}{x \in \Ct_1} {=} \formula{\phi_1} \quad
  \ctransmin{\Sigma}{\Gamma,x}{e\;x \notin \Ct_2} {=} \formula{\phi_2}
  \end{array} 
  ------------------------------------------{NCArr}
  \begin{array}{l} 
  \ctransmin{\Sigma}{\Gamma}{e \notin (x{:}\Ct_1) -> \Ct_2} = 
  \formula{\exists x @.@ \phi_1 \land \phi_2}
  \end{array}
  ~~~~~
  \begin{array}{c}
  \ctransmin{\Sigma}{\Gamma}{e \notin \Ct_1} = \formula{ \phi_1} \quad
  \ctransmin{\Sigma}{\Gamma}{e \notin \Ct_2} = \formula{ \phi_2}
  \end{array}
  ------------------------------------------{NCConj}
  \ctransmin{\Sigma}{\Gamma}{e \in \Ct_1 \& \Ct_2} = \formula{ \phi_1 \lor \phi_2}
  ~~~~~
  \etrans{\Sigma}{\Gamma}{e} =  \formula{t}
  -------------------------------------------{NCCf}
  \ctransmin{\Sigma}{\Gamma}{e \notin \CF} = \formula{\highlight{min(t)} \land \neg\lcf{t}}
 \endprooftree \\ \\ 
\ruleform{\Th_\lcfZ^{min}} \\ \\ 
\begin{array}{lll} 
 \textsc{AxCfBU}  & \formula{\lcf{\unr} /\ \lncf{\bad}} \\
 \textsc{AxCfMin} & \formula{\highlight{\forall x @.@ \lcf{x} => min(x) \lor x = unr}} \\
 %% \textsc{AxCfB1}   & \formula{\forall \oln{x}{n} @.@ \bigwedge_i (\lcf{x_i}\lor \neg(min(x_i))} => \lcf{K(\ol{x})} \lor \neg(min(K(\ol{x}))) \\
 %%                   & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\ 
 \textsc{AxCfC1} & \formula{\forall \oln{x}{n} @.@ \bigwedge\lcf{\ol{x}}} => \lcf{K(\ol{x})} \\
                 & \text{ for every } (K{:}\forall\as @.@ \oln{\tau}{n} -> T\;\as) \in \Sigma \\
 \textsc{AxCfC2} & \formula{\highlight{min(K(\oln{x}{n}))\land\neg\lcf{K(\oln{x}{n})}}} \\ 
                 & \formula{\quad\qquad\qquad \highlight{ => \bigvee_i (min(x_i)\land\neg\lcf{x_i})}}
\end{array}
\end{array}\]
\caption{Translation of contracts with minimization}\label{fig:min-typing}
\end{figure}

Can we {\em guarantee} formally in some cases the existence of a finite model of the theory $\Th_{infty}^{min}$?
The answer is yes for many practically important cases. We may define formally a variation on evaluation, 
$P |- e \SDownarrow v$ which is stricter than $P |- e \Downarrow$ in evaluating {\em all} subterms of a term.
For instance it includes a rule:
\[\begin{array}{c}\prooftree
P |- \highlight{\ol{e} \SDownarrow \ol{v}}
-------------------------------------{EValC}
P |- K[\taus](\ol{e}) \SDownarrow K[\taus](\ol{e})
\endprooftree\end{array}\]
Notice that if $P |- e \SDownarrow v$ then $P |- e \Downarrow v$ (A precise formulation of $P |- e \SDownarrow v$ 
can be found in the Appendix). Observe that despite being stricter, the $\SDownarrow$ relation does not cause more 
@BAD@ values to be thrown.

%% Our finite model theorem is the following:
%% \begin{theorem}\label{thm:finite-model} If $P |- e \SDownarrow v$ then there exists 
%% a finite set $S^{min}$ and an interpretation ${\cal I}^{min}$ such that 
%% $\langle S^{min},{\cal I}^{min}\rangle \models \ThMin$. Moreover 
%% $\langle S^{min},{\cal I}^{min}\rangle \models \ptrans{}{P}^{min}$. 
%% \end{theorem}
We can, for evaluations in $\SDownarrow$, guarantee the existence of finite models. 
\begin{theorem}\label{thm:finite-model} If $P |- e \SDownarrow v$ and 
$P |- e'[e/x] \SDownarrow @BAD@$ or $P |- e'[e/x] \SDownarrow False$, then 
there exists a finite set $S^{min}$ and an interpretation ${\calI}^{min}$ such that 
\[ \langle S^{min},{\calI}^{min}\rangle \models \ThMin \land \Th_{\lcfZ}^{min} \land \ptrans{}{P}^{min} \]
and moreover $\langle S^{min},{\calI}^{min}\rangle \models \ctransmin{}{}{e \notin \{ x \mid e' \}}$.
\end{theorem}
Intuitively the guarantee for a finite model is for a counterexample trace that does not involve some
infinite value (because that would not belong in the $\SDownarrow$ evaluation). We believe that this 
is possible to generalize to arbitrary traces though this might require further changes in our 
$min$-enabled translation. Although the theorem is not the most general theorem that one could desire, 
in practice we have observed that we always get finite countermodels from satisfiable queries in 
our testsuite.

%% However, we are interested in finite models of the theory $\Th_\infty^{min}$ and in what 
%% follows we show how to construct a finite model of $\ThMin$, starting from an execution 
%% trace $P |- e \Downarrow v$ that satisfies certain conditions. 

%% Consider the graph $(G,E)$ induced by an execution trace 
%% $P |- e \Downarrow v$ with $G$ the set 
%%     \[ \{ e \mid P |- e \Downarrow v \text{in the trace}\} \cup 
%%            \{ @bad@ \} \cup \{ @bot@ \} \] 
%% where @bad@ and @bot@ are two distinguished elements (that will serve as the interpretations
%% of $\bad$ and $\unr$ respectively in this model. We also add blue edges $G$ between 
%% between the conclusion and the assumptions of any evaluation rule that has been used 
%% in the trace. Next we {\em complete} this graph so that for every node of the 
%% form $n : (e_1\;e_2)$, if $P |- e_2 \not\Downarrow$, a directed black edge is added 
%% from $e_1\;e_2$ to @bot@, else if $P |- e_2 \Downarrow v_2$ we add a black edge to 
%% from $n : (e_1\;e_2)$ to a new node $e_2$ and recursively build the graph
%% $P |- e_2 \Downarrow v_2$. Similarly for every node of the form $n : K[\taus](\oln{e})$.
%% This process is infinite but it has an infinite fixpoint by Tarski-Knaster since we are
%% continuously adding nodes and edges. 

%% Next we add red undirected edges, along the evaluation blue edges when the semantics
%% agree. We also add red edge for every two application nodes $n : e_1\;e_2$ and 
%% $n' : e_1'\;e_2'$ such that $n_1 : e_1$ and $n_1' : e_1'$ are in the reflexive transitive %% closure of red edges and $n_2 
%% we recurse on to
%% $\oln{e}$ 
%% %%  such that for every rule of the form 
%% %% $\frac{e_1 \Downarrow

%% Let us revisit the 
%% evaluation relation of Figure~\ref{fig:opsem} and let us refine it with the highlighted
%% parts in Figure~\ref{fig:opsem-strict}. The omitted rules are the same as in 
%% Figure~\ref{fig:opsem}, with the only difference that they use $\SDownarrow$ instead
%% of $\Downarrow$.

%% \begin{figure}\small
%% \[\begin{array}{c}
%% \ruleform{P |- u \SDownarrow v} \\ \\
%% \prooftree
%% \begin{array}{c} \ \\ 
%% \end{array}
%% P |- \highlight{\ol{e} \SDownarrow \ol{v}} 
%% -------------------------------------{EValF}
%% P |- f^\ar[\taus]\;\oln{e}{m < \ar} \SDownarrow f^\ar[\taus]\;\oln{e}{m < \ar}
%% ~~~~~
%% P |- \highlight{\ol{e} \SDownarrow \ol{v}}
%% -------------------------------------{EValC}
%% P |- K[\taus](\ol{e}) \SDownarrow K[\taus](\ol{e})
%% ~~~~
%% \phantom{G}
%% -------------------------------------{EValB}
%% P |- @BAD@ \SDownarrow @BAD@
%% ~~~~~
%% \begin{array}{c}
%% (f |-> \Lambda\ol{a} @.@ \lambda\oln{x{:}\tau}{m} @.@ u) \in P \\
%% P |- u[\ol{\tau}/\ol{a}][\ol{e}/\ol{x}] \SDownarrow v \quad
%% \highlight{P |- \ol{e} \SDownarrow \ol{v}}
%% \end{array}
%% -------------------------------------{EFun}
%% P |- f[\ol{\tau}]\;\oln{e}{m} \SDownarrow v
%% ~~~~~
%% \begin{array}{c}  
%% P |- e_1 \SDownarrow v_1 \\ 
%% \highlight{P |- e_2 \SDownarrow v_2} \\
%% P |- v_1\;e_2 \SDownarrow w
%% \end{array}
%% ------------------------------------------------{EApp}
%% P |- e_1\;e_2 \SDownarrow w
%% ~~~~
%% \begin{array}{c}  \ \\ 
%% P |- e_1 \SDownarrow @BAD@ \\
%% \highlight{P |- e_2 \SDownarrow v_2}
%% \end{array}
%% ------------------------------------------------{EBadApp}
%% P |- e_1\;e_2 \SDownarrow @BAD@
%% \endprooftree \\ \\ 
%% \text{ .. plus rules for @case@ .. } 
%% \end{array}\]
%% \caption{Strict operational semantics}\label{fig:opsem-strct}
%% \end{figure}

%% Observe that $\SDownarrow$ is a stricter version of $\Downarrow$, that 
%% can potentially diverge more often than $\Downarrow$ but cannot crash more often, 
%% as the results of evaluating expressions under constructors or arguments of 
%% applications are not used. In fact the following lemma is a straightforward induction.

%% \begin{lemma}
%% If $P |- e \SDownarrow v$ then $P |- e \Downarrow v$.
%% \end{lemma} 

%% Our finite model theorem is then the following:

%% \begin{theorem}\label{thm:finite-model} If $P |- e \SDownarrow v$ then there exists 
%% a finite set $S^{min}$ and an interpretation ${\cal I}^{min}$ such that 
%% $\langle S^{min},{\cal I}^{min}\rangle \models \ThMin$. Moreover 
%% $\langle S^{min},{\cal I}^{min}\rangle \models \ptrans{}{P}^{min}$. 
%% \end{theorem}
%% \begin{proof} The reader who is interested in the precise 
%% construction of the model from the trace can consult the 
%% Appendix. 
%% \end{proof}


%% \begin{figure} 
%% {\small
%% \[\setlength{\arraycolsep}{1pt}
%% \end{array}\]}
%% \caption{Crash-freedom with minimization}\label{fig:min-theory}
%% \end{figure}



\subsection{Induction}\label{sect:induction}

An important practical extension is the ability to prove contracts by induction. Unfortunately in 
Haskell, datatypes are not inductive and hence ``ordinary'' induction without some extra care is 
out of the question\dv{cite Dan's master thesis about datatype induction even for Haskell programs?}.
\dv{Do we need an example or is this obvious? Simon?}

Fortunately, we can still perform {\em fixpoint induction} whenever we would like to prove something 
about a new function. Fixpoint induction is justified by the following principle below:

\begin{lemma} Let $\Ct$ be a closed contract -- then $\dbrace{\Ct}$ is an 
admissible predicate. Moreover $\bot \in \dbrace{\Ct}$. \end{lemma}
\begin{proof}
The fact that $\bot$ belongs in every contract can be proven easily by inspection.
Admissibility follows from the fact that (i) the fixpoint $F_\lcfZ^{\infty}$ is admissible, 
and (ii)~the base contracts are admissible since the denotations of Haskell functions 
are continuous.
\end{proof}

In practical terms this means that, given a recursive function $f |-> \Lambda \as @.@ \ol{x{:}\tau} @.@ e[f]$, 
we can prove that $\dbrace{\Ct}(\dbrace{f})$ if we can prove that 
$ \dbrace{P}^{k}(f) \in \dbrace{\Ct}$ implies $\dbrace{P}^{k+1}(f) \in \dbrace{\Ct}$. Moreover, we have that 
$ \dbrace{P}^{0}(f) \in \dbrace{\Ct}$ (corollary of the lemma above), and the induction step above can be 
used to show that all the elements of the infinite chain whose limit is $\dbrace{f}$ are in the denotation of 
the contract $\Ct$. Hence, by admissibility of contract denotations, the limit itself $\dbrace{f} \in \dbrace{\Ct}$. 
We omit further details as they are simply tedious tidying up of definitions. This induction principle is the basis
of our contract checking tool for recursive definitions.
